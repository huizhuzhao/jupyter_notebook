{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import csv\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I joined a new league this year and they have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In your scenario, a person could just not run ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They don't get paid for how much time you spen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I dunno, back before the August update in an A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No, but Toriyama sometimes would draw himself ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body\n",
       "0  I joined a new league this year and they have ...\n",
       "1  In your scenario, a person could just not run ...\n",
       "2  They don't get paid for how much time you spen...\n",
       "3  I dunno, back before the August update in an A...\n",
       "4  No, but Toriyama sometimes would draw himself ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_file = '/home/huizhu/git_test/rnn-tutorial-rnnlm/data/reddit-comments-2015-08.csv'\n",
    "df_reddit = pd.read_csv(reddit_file)\n",
    "df_reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79170 sentences.\n",
      "Found 65751 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'devoted' and appeared 10 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '[u'SENTENCE_START', u'i', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', u'they', u'have', u'different', u'scoring', u'rules', u'than', u'i', u\"'m\", u'used', u'to', u'.', u'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    " \n",
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print \"Reading CSV file...\"\n",
    "with open(reddit_file, 'rb') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    reader.next()\n",
    "    # Split full comments into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print \"Parsed %d sentences.\" % (len(sentences))\n",
    "     \n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    " \n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())\n",
    " \n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    " \n",
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])\n",
    " \n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    " \n",
    "print \"\\nExample sentence: '%s'\" % sentences[0]\n",
    "print \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0]\n",
    " \n",
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79170,), (79170,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6, 3513, 7, 155, 794, 25, 223, 8, 32, 20, 202, 5025, 350, 91, 6, 66, 207, 5, 2]\n",
      "[6, 3513, 7, 155, 794, 25, 223, 8, 32, 20, 202, 5025, 350, 91, 6, 66, 207, 5, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we see the first sentense is\n",
    "\n",
    "x = [0, 6, 3513, 7, 155, 794, 25, 223, 8, 32, 20, 202, 5025, 350, 91, 6, 66, 207, 5, 2]\n",
    "\n",
    "where each number represent one word, and we will vectorize them later (i.e. $x_t$).\n",
    "\n",
    "And \n",
    "\n",
    "y = [6, 3513, 7, 155, 794, 25, 223, 8, 32, 20, 202, 5025, 350, 91, 6, 66, 207, 5, 2, 1]\n",
    "\n",
    "that is to say, our model will generate the next word conditioned on the previous words.\n",
    "\n",
    "The formulars are:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "s_t &= \\tanh(Ux_t + Ws_{t-1}) \\\\\n",
    "o_t &= \\mathrm{softmax}(Vs_t)\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax2D(x):\n",
    "    assert len(x.shape) == 2, 'x.shape: {0} do not match.'.format(x.shape)\n",
    "    x = np.asarray(x, dtype='float32')\n",
    "    dim = x.shape[1]\n",
    "    x_tile = x[:, :, np.newaxis] # expand the last dim\n",
    "    x_tile = np.tile(x_tile, (1, 1, dim)) # repeat the last dim\n",
    "    \n",
    "    x_sub = x[:, np.newaxis, :] # expand the second dim\n",
    "    x_out = x_tile - x_sub\n",
    "    \n",
    "    g = 1. / np.sum(np.exp(-x_out), axis=1)\n",
    "    return g\n",
    "\n",
    "def softmax(x):\n",
    "    assert isinstance(x, np.ndarray), 'type: {0} do not match.'.format(type(x))\n",
    "    \n",
    "    if len(x.shape) == 1:\n",
    "        x_new = np.reshape(x, (1, -1))\n",
    "        g = softmax2D(x_new)\n",
    "        g = g.ravel()\n",
    "    elif len(x.shape) == 2:\n",
    "        g = softmax2D(x)\n",
    "    else:\n",
    "        raise ValueError('Input array have x.shape == {0}, which is not allowed.'.format(x.shape))\n",
    "    return g\n",
    "\n",
    "class RNNNumpy(object):\n",
    "    def __init__(self, word_dim, hidden_dim, bptt_truncate=4):\n",
    "        t1 = time.time()\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        s_1 = np.sqrt(1./word_dim)\n",
    "        s_2 = np.sqrt(1./hidden_dim)\n",
    "        self.U = np.random.uniform(-s_1, s_1, size=(hidden_dim, word_dim))\n",
    "        self.W = np.random.uniform(-s_2, s_2, size=(hidden_dim, hidden_dim))\n",
    "        self.V = np.random.uniform(-s_2, s_2, size=(word_dim, hidden_dim))\n",
    "        t2 = time.time()\n",
    "        print('Initial time: {0}'.format(t2 - t1))\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        t1 = time.time()\n",
    "        T = len(x)\n",
    "        s = np.zeros((T + 1, self.hidden_dim), dtype='float32')\n",
    "        o = np.zeros((T, self.word_dim), dtype='float32')\n",
    "        for t in range(T):\n",
    "            s[t] = np.tanh(self.U[:, x[t]] + np.dot(self.W, s[t-1]))\n",
    "            o[t] = softmax(np.dot(self.V, s[t]))\n",
    "        t2 = time.time()\n",
    "        print('forward time: {0}'.format(t2 - t1))\n",
    "        return o, s\n",
    "    \n",
    "    def predict(self, x):\n",
    "        o, s = self.forward_prop(x)\n",
    "        y_pred = np.argmax(o, axis=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial time: 0.0908930301666\n",
      "[0, 6, 3513, 7, 155, 794, 25, 223, 8, 32, 20, 202, 5025, 350, 91, 6, 66, 207, 5, 2]\n",
      "forward time: 34.0505671501\n"
     ]
    }
   ],
   "source": [
    "rnn = RNNNumpy(vocabulary_size, 100)\n",
    "print(X_train[0])\n",
    "o_sent, s_sent = rnn.forward_prop(X_train[0])\n",
    "#print(o_sent.shape, s_sent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((20, 8000), (21, 100))\n"
     ]
    }
   ],
   "source": [
    "print(o_sent.shape, s_sent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 10, 28, 18, 24, 15, 27, 9, 25, 16]\n",
      "[20, 10, 28, 18, 24, 15, 27, 9, 25, 16]\n"
     ]
    }
   ],
   "source": [
    "print([len(x) for x in X_train[:10]])\n",
    "print([len(y) for y in y_train[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_sent_len = 30\n",
    "feat_dim = 100\n",
    "dtype_INT = tf.int32\n",
    "dtype_FLOAT = tf.float32\n",
    "input_tensor = tf.placeholder(shape=(None, max_sent_len), dtype=dtype_INT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNTF(object):\n",
    "    def __init__(self, embedded_words, ):\n",
    "        pass\n",
    "\n",
    "    \n",
    "def embedding(input_tensor, vocab_size, word_dim):\n",
    "    \"\"\"\n",
    "    inputs\n",
    "    -------\n",
    "    input_tensor: 2D tensor with shape=(batch_size, max_sentence_len), indices of words\n",
    "    vocab_size: integer, total num of unqiue words, max(input_tensor) == vocab_size - 1\n",
    "    word_dim: integer, length of vectorized word\n",
    "    \n",
    "    return\n",
    "    -------\n",
    "    embedded_words: 3D tensor with shape=(batch_size, max_sentence_len, word_dim)\n",
    "    \"\"\"\n",
    "    shape = (vocab_size, word_dim)\n",
    "    scale = np.sqrt(1./word_dim)\n",
    "    initializer = tf.random_uniform_initializer(-scale, scale, dtype=dtype_FLOAT)\n",
    "    \n",
    "    with tf.variable_scope('embedding'):\n",
    "        params = tf.get_variable('params', shape, initializer=initializer)\n",
    "    embedded_words = tf.gather(params, input_tensor)\n",
    "    \n",
    "    return embedded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Gather_1:0' shape=(?, 30, 100) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_words = embedding(input_tensor, max_sent_len, feat_dim)\n",
    "embedded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot infer num from shape (?, 30, 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c29593b888e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munstack_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0munstack_sent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/huizhu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36munstack\u001b[0;34m(value, num, axis, name)\u001b[0m\n\u001b[1;32m    976\u001b[0m       \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot infer num from shape %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalue_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot infer num from shape (?, 30, 100)"
     ]
    }
   ],
   "source": [
    "unstack_sent = tf.unstack(embedded_words)\n",
    "unstack_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
