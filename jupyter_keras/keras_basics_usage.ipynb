{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_shape, output_shape\n",
    "Dense, LSTM, Embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution2D, Convolution1D\n",
    "from keras.layers import Embedding, LSTM, SimpleRNN, TimeDistributed\n",
    "from keras.layers import MaxPooling2D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((None, 784), (None, 20))\n",
      "((None, 784), (None, 20))\n",
      "(22, 20)\n",
      "(22, 20)\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential(\n",
    "    [Dense(output_dim = 20, input_shape = (784, ))])\n",
    "model_2 = Sequential(\n",
    "    [Dense(output_dim = 20, input_dim = 784)])\n",
    "\n",
    "x = np.random.uniform(size=(22, 784))\n",
    "# Dense layer: \n",
    "##  input_shape  (nb_samples, input_dim)\n",
    "##  output_shape (nb_samples, output_dim)\n",
    "print (model_1.input_shape, model_1.output_shape)\n",
    "print (model_2.input_shape, model_2.output_shape)\n",
    "print (model_1.layers[0].get_output_shape_for((22, 33)))\n",
    "output = model_1.predict(x)\n",
    "print (output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((None, None), (None, None, 150))\n",
      "(100, 3, 150)\n"
     ]
    }
   ],
   "source": [
    "word_vec_dim = 150\n",
    "voca_size = 2000\n",
    "n_samples = 100\n",
    "seq_length = 3\n",
    "model_1 = Sequential([Embedding(output_dim = word_vec_dim, input_dim = voca_size)])\n",
    "# Embedding layer:\n",
    "## input_shape  (nb_samples, sequence_length)\n",
    "## output_shape (nb_samples, sequence_length, output_dim)\n",
    "print (model_1.layers[0].input_shape, model_1.layers[0].output_shape)\n",
    "x = np.random.randint(low=0, high=2000, size=(n_samples, seq_length))\n",
    "output = model_1.predict(x)\n",
    "print (output.shape) # (n_samples, seq_length, word_vec_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((None, 10, 50), (None, 10, 150))\n"
     ]
    }
   ],
   "source": [
    "input_dim = 50\n",
    "output_dim = 150\n",
    "seq_length = 10\n",
    "model = Sequential()\n",
    "#model.add(SimpleRNN(output_dim=output_dim, input_dim=input_dim, input_length=seq_length, return_sequences=True))\n",
    "model.add(SimpleRNN(output_dim=output_dim, input_shape=(seq_length, input_dim), return_sequences=True))\n",
    "# SimpleRNN\n",
    "## input_shape  (nb_samples, timesteps, input_dim)\n",
    "## output_shape (nb_samples, timesteps, output_dim)\n",
    "print (model.input_shape, model.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 10, 150)\n",
      "(None, 300)\n",
      "(20, 300)\n",
      "(20, 10, 300)\n"
     ]
    }
   ],
   "source": [
    "input_dim = 150\n",
    "hidden_dim = 300\n",
    "seq_length = 10\n",
    "n_samples = 20\n",
    "\n",
    "model_1 = Sequential([LSTM(output_dim = hidden_dim, input_dim = input_dim, input_length = seq_length)])\n",
    "# LSTM layer\n",
    "## input_shape  (nb_samples, timesteps, input_dim)\n",
    "## output_shape: \n",
    "####          return_sequences==True: (nb_samples, timesteps, input_dim) \n",
    "####          return_sequences==False: (nb_samples, input_dim) ### only the last output returned\n",
    "print (model_1.layers[0].input_shape)\n",
    "print (model_1.layers[0].output_shape)\n",
    "x = np.random.uniform(size=(n_samples, seq_length, input_dim))\n",
    "output = model_1.predict(x)\n",
    "print (output.shape)\n",
    "\n",
    "model_2 = Sequential([LSTM(output_dim = hidden_dim, input_dim = input_dim, input_length = seq_length, \n",
    "                           return_sequences=True)]) ## return sequences\n",
    "output = model_2.predict(x)\n",
    "print (output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wrapper: TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((None, 10), (None, 10, 200))\n"
     ]
    }
   ],
   "source": [
    "voca_size = 1000\n",
    "seq_length = 10\n",
    "word_vec_dim = 100\n",
    "hidden_dim = 300\n",
    "output_dim = 200\n",
    "model = Sequential()\n",
    "model.add(Embedding(output_dim=word_vec_dim, input_dim=voca_size, input_length=seq_length))\n",
    "model.add(SimpleRNN(output_dim=hidden_dim, activation='sigmoid', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(output_dim=output_dim, activation='softmax')))\n",
    "print (model.input_shape, model.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Convolution2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape: (None, 3, 256, 256)\n",
      "output_shape: (1, 64, 128, 128)\n",
      "conv layer: weight_shape: (64, 3, 5, 5), bias_shape: (64,)\n"
     ]
    }
   ],
   "source": [
    "nb_filter = 64\n",
    "rf_size = (5, 5) # receptive field size\n",
    "input_shape = (3, 256, 256) # 256x256 RGB picture\n",
    "strides = (2, 2)\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(nb_filter=nb_filter, nb_row=rf_size[0], nb_col=rf_size[1], input_shape=input_shape, \n",
    "                                 border_mode='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Convolution2D layer\n",
    "## dim_ordering == 'th'\n",
    "####   input_shape  (nb_samples, channels, rows, cols)\n",
    "####   output_shape (nb_samples, nb_filter, new_rows, new_cols)\n",
    "\n",
    "## dim_ordering == 'tf'\n",
    "####   input_shape  (nb_samples, rows, cols, channels)\n",
    "####   output_shape (nb_samples, new_rows, new_cols, nb_filter)\n",
    "\n",
    "print ('input_shape: {0}'.format(model.layers[0].input_shape))\n",
    "x = np.random.uniform(size=(1, 3, 256, 256))\n",
    "\n",
    "output = model.predict(x)\n",
    "print ('output_shape: {0}'.format(output.shape))\n",
    "ws = model.layers[0].get_weights()\n",
    "print ('conv layer: weight_shape: {0}, bias_shape: {1}'.format(ws[0].shape, ws[1].shape)) # kernel shape (nb_filters, nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape: (None, None, 32)\n",
      "output_shape: (1, 8, 64)\n",
      "weight_shape: (64, 32, 3, 1), bias_shape: (64,)\n"
     ]
    }
   ],
   "source": [
    "nb_filter = 64\n",
    "rf_size = 3\n",
    "input_dim = 32 ## channels to Convolution2D\n",
    "model = Sequential([Convolution1D(nb_filter=nb_filter, filter_length=rf_size, input_dim=32)])\n",
    "# Convolution1D\n",
    "## input_shape  (nb_samples, timesteps, channels)\n",
    "## output_shape (nb_samples, new_timesteps, nb_filter)\n",
    "print ('input_shape: {0}'.format(model.input_shape))\n",
    "x = np.random.uniform(size=(1, 10, input_dim))\n",
    "output = model.predict(x)\n",
    "print ('output_shape: {0}'.format(output.shape))\n",
    "ws = model.layers[0].get_weights()\n",
    "print ('weight_shape: {0}, bias_shape: {1}'.format(ws[0].shape, ws[1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
