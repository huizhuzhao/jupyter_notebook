{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = 6000\n",
    "input_dim = 784\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "model = Sequential(name='test1')\n",
    "model.add(Dense(output_dim=hidden_dim, input_dim=input_dim, name='First_layer'))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=output_dim, name='Second_layer'))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_transformer_3(x, n): \n",
    "    if type(x) == list:\n",
    "        x = np.asarray(x, dtype=np.int).reshape(len(x), 1)\n",
    "    class_ind = [x==class_num for class_num in range(n)]\n",
    "    o_h = np.asarray(np.hstack(class_ind), dtype=np.float32)\n",
    "    return o_h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(123)\n",
    "x = rng.uniform(size=(n_samples, input_dim))\n",
    "y = rng.randint(low=0, high=10, size=(n_samples, 1))\n",
    "y = one_hot_transformer_3(y, output_dim)\n",
    "print (y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 0s - loss: 2.3524 - acc: 0.0953     \n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 0s - loss: 2.3197 - acc: 0.1087     \n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 0s - loss: 2.3068 - acc: 0.1108     \n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 0s - loss: 2.2979 - acc: 0.1232     \n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 0s - loss: 2.2912 - acc: 0.1242     \n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 0s - loss: 2.2831 - acc: 0.1355     \n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 0s - loss: 2.2750 - acc: 0.1422     \n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 0s - loss: 2.2684 - acc: 0.1520     \n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 0s - loss: 2.2605 - acc: 0.1562     \n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 0s - loss: 2.2529 - acc: 0.1632     \n",
      "10/10 [==============================] - 0s\n",
      "[2.0456421375274658, 0.40000000596046448]\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model.fit(x, y, nb_epoch=10, batch_size=32)\n",
    "loss_and_metrics = model.evaluate(x[:10], y[:10])\n",
    "print (loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n"
     ]
    }
   ],
   "source": [
    "print model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"class_name\": \"Sequential\", \"keras_version\": \"1.0.7\", \"config\": [{\"class_name\": \"Dense\", \"config\": {\"W_constraint\": null, \"b_constraint\": null, \"name\": \"First_layer\", \"output_dim\": 100, \"activity_regularizer\": null, \"trainable\": true, \"init\": \"glorot_uniform\", \"bias\": true, \"input_dtype\": \"float32\", \"input_dim\": 784, \"b_regularizer\": null, \"W_regularizer\": null, \"activation\": \"linear\", \"batch_input_shape\": [null, 784]}}, {\"class_name\": \"Activation\", \"config\": {\"activation\": \"relu\", \"trainable\": true, \"name\": \"activation_5\"}}, {\"class_name\": \"Dense\", \"config\": {\"W_constraint\": null, \"b_constraint\": null, \"name\": \"Second_layer\", \"activity_regularizer\": null, \"trainable\": true, \"init\": \"glorot_uniform\", \"bias\": true, \"input_dim\": null, \"b_regularizer\": null, \"W_regularizer\": null, \"activation\": \"linear\", \"output_dim\": 10}}, {\"class_name\": \"Activation\", \"config\": {\"activation\": \"softmax\", \"trainable\": true, \"name\": \"activation_6\"}}]} <type 'str'>\n",
      "<keras.models.Sequential object at 0x7fbec33b5090>\n",
      "sequential_16\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "import json\n",
    "\n",
    "json_string = model.to_json()\n",
    "print json_string, type(json_string)\n",
    "json_string = json.loads(json_string)\n",
    "json_string['model_name'] = 'test_model'\n",
    "json_string = json.dumps(json_string)\n",
    "\n",
    "model2 = model_from_json(json_string)\n",
    "print model2\n",
    "print model2.name\n",
    "#dic = json.loads(json_string)\n",
    "#if not dic.get('model_name', None):\n",
    "#    dic.update({'model_name': 'test_model'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method train_on_batch in module keras.models:\n",
      "\n",
      "train_on_batch(self, x, y, class_weight=None, sample_weight=None, **kwargs) method of keras.models.Sequential instance\n",
      "    Single gradient update over one batch of samples.\n",
      "    \n",
      "    # Arguments\n",
      "        x: input data, as a Numpy array or list of Numpy arrays\n",
      "            (if the model has multiple inputs).\n",
      "        y: labels, as a Numpy array.\n",
      "        class_weight: dictionary mapping classes to a weight value,\n",
      "            used for scaling the loss function (during training only).\n",
      "        sample_weight: sample weights, as a Numpy array.\n",
      "    \n",
      "    # Returns\n",
      "        Scalar training loss (if the model has no metrics)\n",
      "        or list of scalars (if the model computes other metrics).\n",
      "        The attribute `model.metrics_names` will give you\n",
      "        the display labels for the scalar outputs.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print help(model.train_on_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
