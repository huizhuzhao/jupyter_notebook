{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN for atis -- fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Teemo.examples.atis import load_data\n",
    "import numpy as np\n",
    "from assertpy import assert_that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set, valid_set, dicts = load_data.atis()\n",
    "w2idx, labels2idx = dicts['words2idx'], dicts['labels2idx']\n",
    "train_x, _, train_y = train_set\n",
    "valid_x, _, valid_y = valid_set\n",
    "\n",
    "idx2w = {w2idx[k]:k for k in w2idx}\n",
    "idx2labels = {labels2idx[k]:k for k in labels2idx}\n",
    "train_words = [list(map(lambda x: idx2w[x], w)) for w in train_x]\n",
    "train_labels = [list(map(lambda x: idx2labels[x], w)) for w in train_y]\n",
    "valid_words = [list(map(lambda x: idx2w[x], w)) for w in valid_x]\n",
    "valid_labels = [list(map(lambda x: idx2labels[x], w)) for w in valid_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max idx: 571, min idx: 0\n",
      "max idx: 570, min idx: 0\n",
      "max_seq_length: 46\n",
      "max_seq_length: 30\n",
      "max idx: 126, min idx: 0\n",
      "max idx: 126, min idx: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(126, 0)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_max_min_idx(list_of_seq):\n",
    "    min_seq = [np.min(x) for x in list_of_seq]\n",
    "    max_seq = [np.max(x) for x in list_of_seq]\n",
    "    print ('max idx: {0}, min idx: {1}'.format(np.max(max_seq), np.min(min_seq)))\n",
    "    return np.max(max_seq), np.min(min_seq)\n",
    "\n",
    "def get_max_seq_length(list_of_seq):\n",
    "    len_seq = [len(x) for x in list_of_seq]\n",
    "    max_len = np.max(len_seq)\n",
    "    print ('max_seq_length: {0}'.format(max_len))\n",
    "    return max_len\n",
    "\n",
    "def mask_zero_add_1(list_of_seq):\n",
    "    new_list_of_seq = []\n",
    "    for seq in list_of_seq:\n",
    "        seq += 1\n",
    "        new_list_of_seq.append(seq)\n",
    "    return new_list_of_seq\n",
    "\n",
    "def convert_seq_to_matrix(list_of_seq, max_seq_length=50):\n",
    "    new_seq = []\n",
    "    for seq in list_of_seq:\n",
    "        assert_that(len(seq)).is_less_than(max_seq_length)\n",
    "        seq = list(seq) + [0] * (max_seq_length-len(seq))\n",
    "        new_seq.append(seq)        \n",
    "    return np.vstack(new_seq)\n",
    "\n",
    "get_max_min_idx(train_x)\n",
    "get_max_min_idx(valid_x)\n",
    "get_max_seq_length(train_x)\n",
    "get_max_seq_length(valid_x)\n",
    "get_max_min_idx(train_y)\n",
    "get_max_min_idx(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max idx: 572, min idx: 1\n",
      "max idx: 571, min idx: 1\n",
      "((4978, 50), (4978, 50, 127))\n",
      "((893, 50), (893, 50, 127))\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 150\n",
    "\n",
    "train_x_new = mask_zero_add_1(train_x) ## add 1 to every idx, as mask_zero==True\n",
    "valid_x_new = mask_zero_add_1(valid_x)\n",
    "get_max_min_idx(train_x_new)\n",
    "get_max_min_idx(valid_x_new)\n",
    "\n",
    "train_x_new = convert_seq_to_matrix(train_x_new)\n",
    "train_y_new = convert_seq_to_matrix(train_y)\n",
    "valid_x_new = convert_seq_to_matrix(valid_x_new)\n",
    "valid_y_new = convert_seq_to_matrix(valid_y)\n",
    "\n",
    "train_y_new = np.eye(n_classes)[train_y_new]\n",
    "valid_y_new = np.eye(n_classes)[valid_y_new]\n",
    "\n",
    "print (train_x_new.shape, train_y_new.shape)\n",
    "print (valid_x_new.shape, valid_y_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voca_size = len(idx2w) + 3\n",
    "n_classes = len(idx2labels)\n",
    "word_vec_dim = 100\n",
    "hidden_dim = 200\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(voca_size, word_vec_dim, hidden_dim, n_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(output_dim=word_vec_dim, input_dim=voca_size, mask_zero=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(SimpleRNN(output_dim=hidden_dim, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(output_dim=n_classes, activation='softmax')))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    print ('model input_shape (nb_samples, seq_length): {0}'.format(model.input_shape))\n",
    "    print ('model output_shape (nb_samples, seq_length, output_dim): {0}'.format(model.output_shape))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model input_shape (nb_samples, seq_length): (None, None)\n",
      "model output_shape (nb_samples, seq_length, output_dim): (None, None, 127)\n"
     ]
    }
   ],
   "source": [
    "model = build_model(voca_size, word_vec_dim, hidden_dim, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4978/4978 [==============================] - 8s - loss: 1.2877 - acc: 0.1747     \n",
      "Epoch 2/10\n",
      "4978/4978 [==============================] - 9s - loss: 0.4027 - acc: 0.2081     \n",
      "Epoch 3/10\n",
      "4978/4978 [==============================] - 9s - loss: 0.2149 - acc: 0.2171     \n",
      "Epoch 4/10\n",
      "4978/4978 [==============================] - 8s - loss: 0.1482 - acc: 0.2204     \n",
      "Epoch 5/10\n",
      "4978/4978 [==============================] - 8s - loss: 0.1124 - acc: 0.2230     \n",
      "Epoch 6/10\n",
      "4978/4978 [==============================] - 8s - loss: 0.0913 - acc: 0.2243     \n",
      "Epoch 7/10\n",
      "4978/4978 [==============================] - 8s - loss: 0.0753 - acc: 0.2255     \n",
      "Epoch 8/10\n",
      "4978/4978 [==============================] - 10s - loss: 0.0655 - acc: 0.2262    \n",
      "Epoch 9/10\n",
      "4978/4978 [==============================] - 9s - loss: 0.0563 - acc: 0.2265     \n",
      "Epoch 10/10\n",
      "4978/4978 [==============================] - 8s - loss: 0.0497 - acc: 0.2274     \n"
     ]
    }
   ],
   "source": [
    "def fit_model(model, train_x, train_y):\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "fit_model(model, train_x_new, train_y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 16, 13, 16, 17, 16, 11, 17, 17, 13]\n",
      "[19, 16, 13, 16, 17, 16, 11, 17, 17, 13]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(valid_x_new)\n",
    "pred = np.argmax(pred, axis=-1)\n",
    "pred_values_list = [x[:len(y)] for x, y in zip(pred, valid_y)]\n",
    "print ([len(x) for x in pred_values_list[:10]])\n",
    "print ([len(x) for x in valid_y[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision=92.28, Recall = 91.92, F1 = 92.1\n"
     ]
    }
   ],
   "source": [
    "def conlleval_fun(words_list, pred_values_list, true_values_list, idx2labels):\n",
    "    pred_labels_list = [list(map(lambda x: idx2labels[x], seq)) for seq in pred_values_list]\n",
    "    true_labels_list = [list(map(lambda x: idx2labels[x], seq)) for seq in true_values_list]\n",
    "    from Teemo.examples.atis.conlleval import conlleval\n",
    "    con_dict = conlleval(pred_labels_list, true_labels_list, words_list, 'measure.txt')\n",
    "    print ('Precision={}, Recall = {}, F1 = {}'.format(con_dict['r'], con_dict['p'], con_dict['f1']))\n",
    "\n",
    "conlleval_fun(valid_words, pred_values_list, valid_y, idx2labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            precision   recall      f_measure   support     \n",
      "class 0     0.8421      0.4848      0.6154      33          \n",
      "class 1     1.0         0.5588      0.717       34          \n",
      "class 2     0.9118      0.9208      0.9163      101         \n",
      "class 3     0.6667      0.4444      0.5333      9           \n",
      "class 4     0.7778      0.3333      0.4667      21          \n",
      "class 5     0.0         0.0         0.0         2           \n",
      "class 6     0.75        0.5455      0.6316      11          \n",
      "class 7     0.625       0.8333      0.7143      6           \n",
      "class 8     1.0         0.6667      0.8         6           \n",
      "class 9     0.0         0.0         0.0         0           \n",
      "class 10    0.8889      1.0         0.9412      8           \n",
      "class 11    0.0         0.0         0.0         0           \n",
      "class 12    0.8571      1.0         0.9231      6           \n",
      "class 13    1.0         1.0         1.0         8           \n",
      "class 14    0.9429      0.9706      0.9565      34          \n",
      "class 15    0.871       0.871       0.871       31          \n",
      "class 16    0.0         0.0         0.0         1           \n",
      "class 17    0.8485      0.4912      0.6222      57          \n",
      "class 18    0.8889      1.0         0.9412      24          \n",
      "class 19    0.0         0.0         0.0         1           \n",
      "class 20    1.0         1.0         1.0         6           \n",
      "class 21    1.0         0.973       0.9863      37          \n",
      "class 22    0.0         0.0         0.0         2           \n",
      "class 23    0.0         0.0         0.0         0           \n",
      "class 24    0.0         0.0         0.0         1           \n",
      "class 25    0.8947      1.0         0.9444      17          \n",
      "class 26    0.9587      0.9858      0.9721      212         \n",
      "class 27    0.9811      0.9455      0.963       55          \n",
      "class 28    0.9483      0.9821      0.9649      56          \n",
      "class 29    1.0         0.8889      0.9412      9           \n",
      "class 30    1.0         1.0         1.0         3           \n",
      "class 31    1.0         0.6667      0.8         3           \n",
      "class 32    1.0         0.6         0.75        5           \n",
      "class 33    0.968       0.9308      0.949       130         \n",
      "class 34    1.0         1.0         1.0         3           \n",
      "class 35    0.8382      1.0         0.912       57          \n",
      "class 36    0.9692      0.9692      0.9692      65          \n",
      "class 37    1.0         1.0         1.0         6           \n",
      "class 38    0.0         0.0         0.0         2           \n",
      "class 39    0.8889      0.9412      0.9143      17          \n",
      "class 40    0.0         0.0         0.0         1           \n",
      "class 41    0.8333      1.0         0.9091      10          \n",
      "class 42    0.913       0.875       0.8936      24          \n",
      "class 43    0.9167      1.0         0.9565      11          \n",
      "class 44    1.0         1.0         1.0         21          \n",
      "class 45    0.2         1.0         0.3333      1           \n",
      "class 46    1.0         0.8         0.8889      5           \n",
      "class 47    0.1429      0.1667      0.1538      12          \n",
      "class 48    0.9655      0.9943      0.9797      704         \n",
      "class 49    1.0         1.0         1.0         23          \n",
      "class 50    1.0         0.9412      0.9697      17          \n",
      "class 51    0.9412      1.0         0.9697      16          \n",
      "class 52    0.0         0.0         0.0         1           \n",
      "class 53    1.0         0.8         0.8889      10          \n",
      "class 54    0.0         0.0         0.0         2           \n",
      "class 55    0.0         0.0         0.0         0           \n",
      "class 56    0.4286      1.0         0.6         3           \n",
      "class 57    1.0         0.25        0.4         4           \n",
      "class 58    0.75        0.75        0.75        4           \n",
      "class 59    0.0         0.0         0.0         3           \n",
      "class 60    0.0         0.0         0.0         2           \n",
      "class 61    0.0         0.0         0.0         0           \n",
      "class 62    0.0         0.0         0.0         0           \n",
      "class 63    0.0         0.0         0.0         0           \n",
      "class 64    0.0         0.0         0.0         0           \n",
      "class 65    0.0         0.0         0.0         0           \n",
      "class 66    0.9863      0.9863      0.9863      73          \n",
      "class 67    1.0         1.0         1.0         1           \n",
      "class 68    0.0         0.0         0.0         9           \n",
      "class 69    0.0         0.0         0.0         1           \n",
      "class 70    0.0         0.0         0.0         0           \n",
      "class 71    0.9091      1.0         0.9524      20          \n",
      "class 72    0.0         0.0         0.0         0           \n",
      "class 73    0.0         0.0         0.0         0           \n",
      "class 74    0.0         0.0         0.0         0           \n",
      "class 75    0.0         0.0         0.0         0           \n",
      "class 76    0.5         0.5         0.5         4           \n",
      "class 77    0.1667      0.3333      0.2222      3           \n",
      "class 78    0.9673      0.9916      0.9793      716         \n",
      "class 79    0.0         0.0         0.0         1           \n",
      "class 80    1.0         1.0         1.0         18          \n",
      "class 81    0.8519      0.8214      0.8364      28          \n",
      "class 82    1.0         0.9         0.9474      10          \n",
      "class 83    0.913       0.9692      0.9403      65          \n",
      "class 84    0.8462      0.3793      0.5238      29          \n",
      "class 85    0.0         0.0         0.0         0           \n",
      "class 86    0.8889      1.0         0.9412      8           \n",
      "class 87    0.0         0.0         0.0         0           \n",
      "class 88    1.0         1.0         1.0         1           \n",
      "class 89    0.9714      0.9714      0.9714      35          \n",
      "class 90    0.0         0.0         0.0         4           \n",
      "class 91    0.9286      0.4333      0.5909      30          \n",
      "class 92    1.0         1.0         1.0         17          \n",
      "class 93    1.0         0.6667      0.8         3           \n",
      "class 94    1.0         0.9333      0.9655      15          \n",
      "class 95    0.0         0.0         0.0         0           \n",
      "class 96    1.0         0.6667      0.8         3           \n",
      "class 97    0.0         0.0         0.0         1           \n",
      "class 98    1.0         1.0         1.0         1           \n",
      "class 99    0.9455      1.0         0.972       52          \n",
      "class 100   0.0         0.0         0.0         1           \n",
      "class 101   0.0         0.0         0.0         0           \n",
      "class 102   1.0         1.0         1.0         2           \n",
      "class 103   0.0         0.0         0.0         0           \n",
      "class 104   0.0         0.0         0.0         6           \n",
      "class 105   0.0         0.0         0.0         1           \n",
      "class 106   0.0         0.0         0.0         0           \n",
      "class 107   1.0         1.0         1.0         1           \n",
      "class 108   0.48        0.8         0.6         15          \n",
      "class 109   0.9171      1.0         0.9568      177         \n",
      "class 110   1.0         1.0         1.0         1           \n",
      "class 111   0.0         0.0         0.0         0           \n",
      "class 112   0.0         0.0         0.0         0           \n",
      "class 113   1.0         0.6667      0.8         3           \n",
      "class 114   0.0         0.0         0.0         3           \n",
      "class 115   0.0         0.0         0.0         0           \n",
      "class 116   0.0         0.0         0.0         0           \n",
      "class 117   1.0         1.0         1.0         71          \n",
      "class 118   0.0         0.0         0.0         1           \n",
      "class 119   1.0         0.6         0.75        10          \n",
      "class 120   0.0         0.0         0.0         0           \n",
      "class 121   0.0         0.0         0.0         0           \n",
      "class 122   0.6         1.0         0.75        3           \n",
      "class 123   0.9596      0.9849      0.9721      265         \n",
      "class 124   1.0         1.0         1.0         1           \n",
      "class 125   0.0         0.0         0.0         1           \n",
      "class 126   0.9893      0.9978      0.9935      5535        \n",
      "avg/total   0.9659      0.9697      0.9656      9198        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = np.hstack(valid_y)\n",
    "y_pred = np.hstack(pred_values_list)\n",
    "y_true = np.eye(n_classes)[y_true]\n",
    "y_pred = np.eye(n_classes)[y_pred]\n",
    "from Teemo.algorithm.utils.evaluations import classification_evaluate\n",
    "from Teemo.algorithm.utils.report_funcs import classification_report\n",
    "res = classification_evaluate(y_pred, y_true)\n",
    "print (classification_report(res))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
