{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(439863, 130)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcba_file = '/home/xtalpi/bitbucket/mol_data/pcba_deepchem/pcba.csv'\n",
    "df = pd.read_csv(pcba_file)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCBA-1030</th>\n",
       "      <th>PCBA-1379</th>\n",
       "      <th>PCBA-1452</th>\n",
       "      <th>PCBA-1454</th>\n",
       "      <th>PCBA-1457</th>\n",
       "      <th>PCBA-1458</th>\n",
       "      <th>PCBA-1460</th>\n",
       "      <th>PCBA-1461</th>\n",
       "      <th>PCBA-1468</th>\n",
       "      <th>PCBA-1469</th>\n",
       "      <th>...</th>\n",
       "      <th>PCBA-914</th>\n",
       "      <th>PCBA-915</th>\n",
       "      <th>PCBA-924</th>\n",
       "      <th>PCBA-925</th>\n",
       "      <th>PCBA-926</th>\n",
       "      <th>PCBA-927</th>\n",
       "      <th>PCBA-938</th>\n",
       "      <th>PCBA-995</th>\n",
       "      <th>mol_id</th>\n",
       "      <th>smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CID1511280</td>\n",
       "      <td>CC(=O)N1CCC2(CC1)NC(=O)N(c1ccccc1)N2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CID332939</td>\n",
       "      <td>N#Cc1nnn(-c2ccc(Cl)cc2)c1N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CID3800322</td>\n",
       "      <td>COC(=O)c1ccc(NC(=O)c2ccccc2CC[N+](=O)[O-])cc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CID46904422</td>\n",
       "      <td>CCC1NC(=O)c2cccnc2-n2c1nc1ccc(F)cc1c2=O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CID16445987</td>\n",
       "      <td>CC1=CC(=O)/C(=C2/C=C(C(=O)Nc3ccc(S(=O)(=O)Nc4o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PCBA-1030  PCBA-1379  PCBA-1452  PCBA-1454  PCBA-1457  PCBA-1458  \\\n",
       "0        0.0        0.0        NaN        0.0        0.0        0.0   \n",
       "1        0.0        0.0        NaN        0.0        0.0        NaN   \n",
       "2        NaN        0.0        NaN        0.0        0.0        0.0   \n",
       "3        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "4        NaN        NaN        NaN        0.0        NaN        NaN   \n",
       "\n",
       "   PCBA-1460  PCBA-1461  PCBA-1468  PCBA-1469  \\\n",
       "0        0.0        0.0        0.0        0.0   \n",
       "1        0.0        0.0        0.0        0.0   \n",
       "2        0.0        0.0        0.0        0.0   \n",
       "3        NaN        NaN        NaN        NaN   \n",
       "4        NaN        0.0        0.0        0.0   \n",
       "\n",
       "                         ...                          PCBA-914  PCBA-915  \\\n",
       "0                        ...                               NaN       NaN   \n",
       "1                        ...                               NaN       NaN   \n",
       "2                        ...                               NaN       NaN   \n",
       "3                        ...                               NaN       NaN   \n",
       "4                        ...                               NaN       NaN   \n",
       "\n",
       "   PCBA-924  PCBA-925  PCBA-926  PCBA-927  PCBA-938  PCBA-995       mol_id  \\\n",
       "0       NaN       NaN       NaN       NaN       NaN       NaN   CID1511280   \n",
       "1       NaN       NaN       NaN       NaN       NaN       NaN    CID332939   \n",
       "2       NaN       NaN       NaN       NaN       NaN       NaN   CID3800322   \n",
       "3       NaN       NaN       NaN       NaN       NaN       NaN  CID46904422   \n",
       "4       NaN       NaN       NaN       NaN       NaN       NaN  CID16445987   \n",
       "\n",
       "                                              smiles  \n",
       "0               CC(=O)N1CCC2(CC1)NC(=O)N(c1ccccc1)N2  \n",
       "1                         N#Cc1nnn(-c2ccc(Cl)cc2)c1N  \n",
       "2      COC(=O)c1ccc(NC(=O)c2ccccc2CC[N+](=O)[O-])cc1  \n",
       "3            CCC1NC(=O)c2cccnc2-n2c1nc1ccc(F)cc1c2=O  \n",
       "4  CC1=CC(=O)/C(=C2/C=C(C(=O)Nc3ccc(S(=O)(=O)Nc4o...  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deepchem_pcba_file = '/home/xtalpi/bitbucket/mol_data/pcba_deepchem/pcba_temp.csv'\n",
    "df_sub = df.loc[:2000]\n",
    "df_sub.to_csv(deepchem_pcba_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCBA-1030</th>\n",
       "      <th>PCBA-1379</th>\n",
       "      <th>PCBA-1452</th>\n",
       "      <th>PCBA-1454</th>\n",
       "      <th>PCBA-1457</th>\n",
       "      <th>PCBA-1458</th>\n",
       "      <th>PCBA-1460</th>\n",
       "      <th>PCBA-1461</th>\n",
       "      <th>PCBA-1468</th>\n",
       "      <th>PCBA-1469</th>\n",
       "      <th>...</th>\n",
       "      <th>PCBA-914</th>\n",
       "      <th>PCBA-915</th>\n",
       "      <th>PCBA-924</th>\n",
       "      <th>PCBA-925</th>\n",
       "      <th>PCBA-926</th>\n",
       "      <th>PCBA-927</th>\n",
       "      <th>PCBA-938</th>\n",
       "      <th>PCBA-995</th>\n",
       "      <th>mol_id</th>\n",
       "      <th>smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CID1511280</td>\n",
       "      <td>CC(=O)N1CCC2(CC1)NC(=O)N(c1ccccc1)N2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CID332939</td>\n",
       "      <td>N#Cc1nnn(-c2ccc(Cl)cc2)c1N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CID3800322</td>\n",
       "      <td>COC(=O)c1ccc(NC(=O)c2ccccc2CC[N+](=O)[O-])cc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CID46904422</td>\n",
       "      <td>CCC1NC(=O)c2cccnc2-n2c1nc1ccc(F)cc1c2=O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CID16445987</td>\n",
       "      <td>CC1=CC(=O)/C(=C2/C=C(C(=O)Nc3ccc(S(=O)(=O)Nc4o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PCBA-1030  PCBA-1379  PCBA-1452  PCBA-1454  PCBA-1457  PCBA-1458  \\\n",
       "0        0.0        0.0        NaN        0.0        0.0        0.0   \n",
       "1        0.0        0.0        NaN        0.0        0.0        NaN   \n",
       "2        NaN        0.0        NaN        0.0        0.0        0.0   \n",
       "3        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "4        NaN        NaN        NaN        0.0        NaN        NaN   \n",
       "\n",
       "   PCBA-1460  PCBA-1461  PCBA-1468  PCBA-1469  \\\n",
       "0        0.0        0.0        0.0        0.0   \n",
       "1        0.0        0.0        0.0        0.0   \n",
       "2        0.0        0.0        0.0        0.0   \n",
       "3        NaN        NaN        NaN        NaN   \n",
       "4        NaN        0.0        0.0        0.0   \n",
       "\n",
       "                         ...                          PCBA-914  PCBA-915  \\\n",
       "0                        ...                               NaN       NaN   \n",
       "1                        ...                               NaN       NaN   \n",
       "2                        ...                               NaN       NaN   \n",
       "3                        ...                               NaN       NaN   \n",
       "4                        ...                               NaN       NaN   \n",
       "\n",
       "   PCBA-924  PCBA-925  PCBA-926  PCBA-927  PCBA-938  PCBA-995       mol_id  \\\n",
       "0       NaN       NaN       NaN       NaN       NaN       NaN   CID1511280   \n",
       "1       NaN       NaN       NaN       NaN       NaN       NaN    CID332939   \n",
       "2       NaN       NaN       NaN       NaN       NaN       NaN   CID3800322   \n",
       "3       NaN       NaN       NaN       NaN       NaN       NaN  CID46904422   \n",
       "4       NaN       NaN       NaN       NaN       NaN       NaN  CID16445987   \n",
       "\n",
       "                                              smiles  \n",
       "0               CC(=O)N1CCC2(CC1)NC(=O)N(c1ccccc1)N2  \n",
       "1                         N#Cc1nnn(-c2ccc(Cl)cc2)c1N  \n",
       "2      COC(=O)c1ccc(NC(=O)c2ccccc2CC[N+](=O)[O-])cc1  \n",
       "3            CCC1NC(=O)c2cccnc2-n2c1nc1ccc(F)cc1c2=O  \n",
       "4  CC1=CC(=O)/C(=C2/C=C(C(=O)Nc3ccc(S(=O)(=O)Nc4o...  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepchem_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://pubchem.ncbi.nlm.nih.gov/pcajax/pcget.cgi?query=download&record_type=datatable&actvty=all&response_type=save&aid=1030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
=======
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from Erwin.models.tf_utils import tf_core\n",
    "from Erwin.models.tf_utils import models\n",
    "from Erwin.models.mlp.mlp import MLP"
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
<<<<<<< HEAD
=======
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "model.build(**{'input_dim': 784,\n",
    "            'hidden_dim': 100,\n",
    "            'hidden_acti': 'relu',\n",
    "            'output_dim': 10,\n",
    "            'output_acti': 'softmax'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "((200,), (200,))"
      ]
     },
     "execution_count": 5,
=======
       "<tf.Tensor 'Softmax:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 6,
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "X = np.linspace(0, 2.*np.pi, 200)\n",
    "Y = np.sin(X)\n",
    "X.shape, Y.shape"
=======
    "model.output_tensor"
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = model._compile_loss('categorical_cross_entropy', model.output_tensor, model.labels_tensor, model.weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = tf.trainable_variables()\n",
    "grad = tf.gradients(loss, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 31.0\n",
      "Trainable params: 31\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
=======
      "[(784, 100), (100,), (100, 10), (10,)]\n",
      "[(784, 100), (100,), (100, 10), (10,)]\n"
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=1, activation='relu'))\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "model.summary()"
=======
    "print([tf_core.int_shape(w) for w in weights])\n",
    "print([tf_core.int_shape(g) for g in grad])"
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 15,
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "from Erwin.models.tf_utils import metrics\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=[metrics.pearson_corr])"
=======
    "adam = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "grads_and_vars = adam.compute_gradients(loss, weights)"
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
<<<<<<< HEAD
    "collapsed": false,
    "scrolled": true
=======
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'gradients_2/MatMul_grad/tuple/control_dependency_1:0' shape=(784, 100) dtype=float32>,\n",
       " <tensorflow.python.ops.variables.Variable at 0x7f0d1b4ed7d0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_and_vars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(784, 100), (784, 100)],\n",
       " [(100,), (100,)],\n",
       " [(100, 10), (100, 10)],\n",
       " [(10,), (10,)]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[tf_core.int_shape(x) for x in w] for w in grads_and_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([0, 1, 2, 3, 4])\n",
    "y = np.array([1, -1, 2, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 1/200\n",
      "200/200 [==============================] - 0s - loss: 0.1319 - pearson_corr: 0.8597     \n",
      "Epoch 2/200\n",
      "200/200 [==============================] - 0s - loss: 0.1316 - pearson_corr: 0.8595     \n",
      "Epoch 3/200\n",
      "200/200 [==============================] - 0s - loss: 0.1314 - pearson_corr: 0.8580     \n",
      "Epoch 4/200\n",
      "200/200 [==============================] - 0s - loss: 0.1312 - pearson_corr: 0.8602     \n",
      "Epoch 5/200\n",
      "200/200 [==============================] - 0s - loss: 0.1310 - pearson_corr: 0.8622     \n",
      "Epoch 6/200\n",
      "200/200 [==============================] - 0s - loss: 0.1308 - pearson_corr: 0.8592     \n",
      "Epoch 7/200\n",
      "200/200 [==============================] - 0s - loss: 0.1306 - pearson_corr: 0.8594     \n",
      "Epoch 8/200\n",
      "200/200 [==============================] - 0s - loss: 0.1304 - pearson_corr: 0.8602     \n",
      "Epoch 9/200\n",
      "200/200 [==============================] - 0s - loss: 0.1300 - pearson_corr: 0.8622     \n",
      "Epoch 10/200\n",
      "200/200 [==============================] - 0s - loss: 0.1298 - pearson_corr: 0.8611     \n",
      "Epoch 11/200\n",
      "200/200 [==============================] - 0s - loss: 0.1296 - pearson_corr: 0.8614     \n",
      "Epoch 12/200\n",
      "200/200 [==============================] - 0s - loss: 0.1294 - pearson_corr: 0.8592     \n",
      "Epoch 13/200\n",
      "200/200 [==============================] - 0s - loss: 0.1292 - pearson_corr: 0.8588     \n",
      "Epoch 14/200\n",
      "200/200 [==============================] - 0s - loss: 0.1290 - pearson_corr: 0.8581     \n",
      "Epoch 15/200\n",
      "200/200 [==============================] - 0s - loss: 0.1288 - pearson_corr: 0.8603     \n",
      "Epoch 16/200\n",
      "200/200 [==============================] - 0s - loss: 0.1286 - pearson_corr: 0.8639     \n",
      "Epoch 17/200\n",
      "200/200 [==============================] - 0s - loss: 0.1284 - pearson_corr: 0.8618     \n",
      "Epoch 18/200\n",
      "200/200 [==============================] - 0s - loss: 0.1283 - pearson_corr: 0.8637     \n",
      "Epoch 19/200\n",
      "200/200 [==============================] - 0s - loss: 0.1280 - pearson_corr: 0.8639     \n",
      "Epoch 20/200\n",
      "200/200 [==============================] - 0s - loss: 0.1278 - pearson_corr: 0.8616     \n",
      "Epoch 21/200\n",
      "200/200 [==============================] - 0s - loss: 0.1276 - pearson_corr: 0.8643     \n",
      "Epoch 22/200\n",
      "200/200 [==============================] - 0s - loss: 0.1274 - pearson_corr: 0.8651     \n",
      "Epoch 23/200\n",
      "200/200 [==============================] - 0s - loss: 0.1272 - pearson_corr: 0.8632     \n",
      "Epoch 24/200\n",
      "200/200 [==============================] - 0s - loss: 0.1271 - pearson_corr: 0.8609     \n",
      "Epoch 25/200\n",
      "200/200 [==============================] - 0s - loss: 0.1269 - pearson_corr: 0.8654     \n",
      "Epoch 26/200\n",
      "200/200 [==============================] - 0s - loss: 0.1267 - pearson_corr: 0.8634     \n",
      "Epoch 27/200\n",
      "200/200 [==============================] - 0s - loss: 0.1265 - pearson_corr: 0.8628     \n",
      "Epoch 28/200\n",
      "200/200 [==============================] - 0s - loss: 0.1264 - pearson_corr: 0.8679     \n",
      "Epoch 29/200\n",
      "200/200 [==============================] - 0s - loss: 0.1261 - pearson_corr: 0.8657     \n",
      "Epoch 30/200\n",
      "200/200 [==============================] - 0s - loss: 0.1260 - pearson_corr: 0.8672     \n",
      "Epoch 31/200\n",
      "200/200 [==============================] - 0s - loss: 0.1258 - pearson_corr: 0.8647     \n",
      "Epoch 32/200\n",
      "200/200 [==============================] - 0s - loss: 0.1256 - pearson_corr: 0.8663     \n",
      "Epoch 33/200\n",
      "200/200 [==============================] - 0s - loss: 0.1254 - pearson_corr: 0.8710     \n",
      "Epoch 34/200\n",
      "200/200 [==============================] - 0s - loss: 0.1252 - pearson_corr: 0.8718     \n",
      "Epoch 35/200\n",
      "200/200 [==============================] - 0s - loss: 0.1250 - pearson_corr: 0.8638     \n",
      "Epoch 36/200\n",
      "200/200 [==============================] - 0s - loss: 0.1248 - pearson_corr: 0.8685     \n",
      "Epoch 37/200\n",
      "200/200 [==============================] - 0s - loss: 0.1247 - pearson_corr: 0.8663     \n",
      "Epoch 38/200\n",
      "200/200 [==============================] - 0s - loss: 0.1245 - pearson_corr: 0.8673     \n",
      "Epoch 39/200\n",
      "200/200 [==============================] - 0s - loss: 0.1244 - pearson_corr: 0.8683     \n",
      "Epoch 40/200\n",
      "200/200 [==============================] - 0s - loss: 0.1242 - pearson_corr: 0.8663     \n",
      "Epoch 41/200\n",
      "200/200 [==============================] - 0s - loss: 0.1241 - pearson_corr: 0.8626     \n",
      "Epoch 42/200\n",
      "200/200 [==============================] - 0s - loss: 0.1240 - pearson_corr: 0.8719     \n",
      "Epoch 43/200\n",
      "200/200 [==============================] - 0s - loss: 0.1237 - pearson_corr: 0.8703     \n",
      "Epoch 44/200\n",
      "200/200 [==============================] - 0s - loss: 0.1236 - pearson_corr: 0.8750     \n",
      "Epoch 45/200\n",
      "200/200 [==============================] - 0s - loss: 0.1235 - pearson_corr: 0.8706     \n",
      "Epoch 46/200\n",
      "200/200 [==============================] - 0s - loss: 0.1234 - pearson_corr: 0.8688     \n",
      "Epoch 47/200\n",
      "200/200 [==============================] - 0s - loss: 0.1232 - pearson_corr: 0.8647     \n",
      "Epoch 48/200\n",
      "200/200 [==============================] - 0s - loss: 0.1231 - pearson_corr: 0.8698     \n",
      "Epoch 49/200\n",
      "200/200 [==============================] - 0s - loss: 0.1229 - pearson_corr: 0.8693     \n",
      "Epoch 50/200\n",
      "200/200 [==============================] - 0s - loss: 0.1227 - pearson_corr: 0.8676     \n",
      "Epoch 51/200\n",
      "200/200 [==============================] - 0s - loss: 0.1226 - pearson_corr: 0.8715     \n",
      "Epoch 52/200\n",
      "200/200 [==============================] - 0s - loss: 0.1224 - pearson_corr: 0.8711     \n",
      "Epoch 53/200\n",
      "200/200 [==============================] - 0s - loss: 0.1222 - pearson_corr: 0.8761     \n",
      "Epoch 54/200\n",
      "200/200 [==============================] - 0s - loss: 0.1221 - pearson_corr: 0.8675     \n",
      "Epoch 55/200\n",
      "200/200 [==============================] - 0s - loss: 0.1220 - pearson_corr: 0.8696     \n",
      "Epoch 56/200\n",
      "200/200 [==============================] - 0s - loss: 0.1218 - pearson_corr: 0.8686     \n",
      "Epoch 57/200\n",
      "200/200 [==============================] - 0s - loss: 0.1217 - pearson_corr: 0.8696     \n",
      "Epoch 58/200\n",
      "200/200 [==============================] - 0s - loss: 0.1215 - pearson_corr: 0.8700     \n",
      "Epoch 59/200\n",
      "200/200 [==============================] - 0s - loss: 0.1214 - pearson_corr: 0.8618     \n",
      "Epoch 60/200\n",
      "200/200 [==============================] - 0s - loss: 0.1213 - pearson_corr: 0.8767     \n",
      "Epoch 61/200\n",
      "200/200 [==============================] - 0s - loss: 0.1212 - pearson_corr: 0.8704     \n",
      "Epoch 62/200\n",
      "200/200 [==============================] - 0s - loss: 0.1210 - pearson_corr: 0.8707     \n",
      "Epoch 63/200\n",
      "200/200 [==============================] - 0s - loss: 0.1209 - pearson_corr: 0.8726     \n",
      "Epoch 64/200\n",
      "200/200 [==============================] - 0s - loss: 0.1208 - pearson_corr: 0.8720     \n",
      "Epoch 65/200\n",
      "200/200 [==============================] - 0s - loss: 0.1207 - pearson_corr: 0.8757     \n",
      "Epoch 66/200\n",
      "200/200 [==============================] - 0s - loss: 0.1206 - pearson_corr: 0.8716     \n",
      "Epoch 67/200\n",
      "200/200 [==============================] - 0s - loss: 0.1205 - pearson_corr: 0.8724     \n",
      "Epoch 68/200\n",
      "200/200 [==============================] - 0s - loss: 0.1204 - pearson_corr: 0.8721     \n",
      "Epoch 69/200\n",
      "200/200 [==============================] - 0s - loss: 0.1202 - pearson_corr: 0.8697     \n",
      "Epoch 70/200\n",
      "200/200 [==============================] - 0s - loss: 0.1201 - pearson_corr: 0.8716     \n",
      "Epoch 71/200\n",
      "200/200 [==============================] - 0s - loss: 0.1200 - pearson_corr: 0.8695     \n",
      "Epoch 72/200\n",
      "200/200 [==============================] - 0s - loss: 0.1199 - pearson_corr: 0.8750     \n",
      "Epoch 73/200\n",
      "200/200 [==============================] - 0s - loss: 0.1198 - pearson_corr: 0.8717     \n",
      "Epoch 74/200\n",
      "200/200 [==============================] - 0s - loss: 0.1197 - pearson_corr: 0.8730     \n",
      "Epoch 75/200\n",
      "200/200 [==============================] - 0s - loss: 0.1195 - pearson_corr: 0.8735     \n",
      "Epoch 76/200\n",
      "200/200 [==============================] - 0s - loss: 0.1194 - pearson_corr: 0.8722     \n",
      "Epoch 77/200\n",
      "200/200 [==============================] - 0s - loss: 0.1193 - pearson_corr: 0.8786     \n",
      "Epoch 78/200\n",
      "200/200 [==============================] - 0s - loss: 0.1192 - pearson_corr: 0.8764     \n",
      "Epoch 79/200\n",
      "200/200 [==============================] - 0s - loss: 0.1192 - pearson_corr: 0.8757     \n",
      "Epoch 80/200\n",
      "200/200 [==============================] - 0s - loss: 0.1191 - pearson_corr: 0.8705     \n",
      "Epoch 81/200\n",
      "200/200 [==============================] - 0s - loss: 0.1189 - pearson_corr: 0.8720     \n",
      "Epoch 82/200\n",
      "200/200 [==============================] - 0s - loss: 0.1188 - pearson_corr: 0.8751     \n",
      "Epoch 83/200\n",
      "200/200 [==============================] - 0s - loss: 0.1187 - pearson_corr: 0.8757     \n",
      "Epoch 84/200\n",
      "200/200 [==============================] - 0s - loss: 0.1186 - pearson_corr: 0.8737     \n",
      "Epoch 85/200\n",
      "200/200 [==============================] - 0s - loss: 0.1185 - pearson_corr: 0.8747     \n",
      "Epoch 86/200\n",
      "200/200 [==============================] - 0s - loss: 0.1184 - pearson_corr: 0.8761     \n",
      "Epoch 87/200\n",
      "200/200 [==============================] - 0s - loss: 0.1184 - pearson_corr: 0.8790     \n",
      "Epoch 88/200\n",
      "200/200 [==============================] - 0s - loss: 0.1182 - pearson_corr: 0.8755     \n",
      "Epoch 89/200\n",
      "200/200 [==============================] - 0s - loss: 0.1181 - pearson_corr: 0.8740     \n",
      "Epoch 90/200\n",
      "200/200 [==============================] - 0s - loss: 0.1180 - pearson_corr: 0.8788     \n",
      "Epoch 91/200\n",
      "200/200 [==============================] - 0s - loss: 0.1179 - pearson_corr: 0.8787     \n",
      "Epoch 92/200\n",
      "200/200 [==============================] - 0s - loss: 0.1179 - pearson_corr: 0.8774     \n",
      "Epoch 93/200\n",
      "200/200 [==============================] - 0s - loss: 0.1178 - pearson_corr: 0.8730     \n",
      "Epoch 94/200\n",
      "200/200 [==============================] - 0s - loss: 0.1177 - pearson_corr: 0.8751     \n",
      "Epoch 95/200\n",
      "200/200 [==============================] - 0s - loss: 0.1176 - pearson_corr: 0.8800     \n",
      "Epoch 96/200\n",
      "200/200 [==============================] - 0s - loss: 0.1175 - pearson_corr: 0.8750     \n",
      "Epoch 97/200\n",
      "200/200 [==============================] - 0s - loss: 0.1175 - pearson_corr: 0.8741     \n",
      "Epoch 98/200\n",
      "200/200 [==============================] - 0s - loss: 0.1174 - pearson_corr: 0.8732     \n",
      "Epoch 99/200\n",
      "200/200 [==============================] - 0s - loss: 0.1173 - pearson_corr: 0.8740     \n",
      "Epoch 100/200\n",
      "200/200 [==============================] - 0s - loss: 0.1172 - pearson_corr: 0.8779     \n",
      "Epoch 101/200\n",
      "200/200 [==============================] - 0s - loss: 0.1171 - pearson_corr: 0.8760     \n",
      "Epoch 102/200\n",
      "200/200 [==============================] - 0s - loss: 0.1170 - pearson_corr: 0.8768     \n",
      "Epoch 103/200\n",
      "200/200 [==============================] - 0s - loss: 0.1169 - pearson_corr: 0.8753     \n",
      "Epoch 104/200\n",
      "200/200 [==============================] - 0s - loss: 0.1168 - pearson_corr: 0.8777     \n",
      "Epoch 105/200\n",
      "200/200 [==============================] - 0s - loss: 0.1168 - pearson_corr: 0.8775     \n",
      "Epoch 106/200\n",
      "200/200 [==============================] - 0s - loss: 0.1167 - pearson_corr: 0.8776     \n",
      "Epoch 107/200\n",
      "200/200 [==============================] - 0s - loss: 0.1166 - pearson_corr: 0.8813     \n",
      "Epoch 108/200\n",
      "200/200 [==============================] - 0s - loss: 0.1165 - pearson_corr: 0.8772     \n",
      "Epoch 109/200\n",
      "200/200 [==============================] - 0s - loss: 0.1164 - pearson_corr: 0.8723     \n",
      "Epoch 110/200\n",
      "200/200 [==============================] - 0s - loss: 0.1163 - pearson_corr: 0.8791     \n",
      "Epoch 111/200\n",
      "200/200 [==============================] - 0s - loss: 0.1162 - pearson_corr: 0.8800     \n",
      "Epoch 112/200\n",
      "200/200 [==============================] - 0s - loss: 0.1162 - pearson_corr: 0.8793     \n",
      "Epoch 113/200\n",
      "200/200 [==============================] - 0s - loss: 0.1161 - pearson_corr: 0.8783     \n",
      "Epoch 114/200\n",
      "200/200 [==============================] - 0s - loss: 0.1160 - pearson_corr: 0.8770     \n",
      "Epoch 115/200\n",
      "200/200 [==============================] - 0s - loss: 0.1159 - pearson_corr: 0.8808     \n",
      "Epoch 116/200\n",
      "200/200 [==============================] - 0s - loss: 0.1159 - pearson_corr: 0.8805     \n",
      "Epoch 117/200\n",
      "200/200 [==============================] - 0s - loss: 0.1159 - pearson_corr: 0.8738     \n",
      "Epoch 118/200\n",
      "200/200 [==============================] - 0s - loss: 0.1157 - pearson_corr: 0.8810     \n",
      "Epoch 119/200\n",
      "200/200 [==============================] - 0s - loss: 0.1156 - pearson_corr: 0.8811     \n",
      "Epoch 120/200\n",
      "200/200 [==============================] - 0s - loss: 0.1156 - pearson_corr: 0.8791     \n",
      "Epoch 121/200\n",
      "200/200 [==============================] - 0s - loss: 0.1155 - pearson_corr: 0.8830     \n",
      "Epoch 122/200\n",
      "200/200 [==============================] - 0s - loss: 0.1154 - pearson_corr: 0.8845     \n",
      "Epoch 123/200\n",
      "200/200 [==============================] - 0s - loss: 0.1154 - pearson_corr: 0.8756     \n",
      "Epoch 124/200\n",
      "200/200 [==============================] - 0s - loss: 0.1153 - pearson_corr: 0.8821     \n",
      "Epoch 125/200\n",
      "200/200 [==============================] - 0s - loss: 0.1152 - pearson_corr: 0.8839     \n",
      "Epoch 126/200\n",
      "200/200 [==============================] - 0s - loss: 0.1152 - pearson_corr: 0.8785     \n",
      "Epoch 127/200\n",
      "200/200 [==============================] - 0s - loss: 0.1152 - pearson_corr: 0.8800     \n",
      "Epoch 128/200\n",
      "200/200 [==============================] - 0s - loss: 0.1151 - pearson_corr: 0.8768     \n",
      "Epoch 129/200\n",
      "200/200 [==============================] - 0s - loss: 0.1151 - pearson_corr: 0.8824     \n",
      "Epoch 130/200\n",
      "200/200 [==============================] - 0s - loss: 0.1150 - pearson_corr: 0.8767     \n",
      "Epoch 131/200\n",
      "200/200 [==============================] - 0s - loss: 0.1150 - pearson_corr: 0.8776     \n",
      "Epoch 132/200\n",
      "200/200 [==============================] - 0s - loss: 0.1150 - pearson_corr: 0.8793     \n",
      "Epoch 133/200\n",
      "200/200 [==============================] - 0s - loss: 0.1149 - pearson_corr: 0.8805     \n",
      "Epoch 134/200\n",
      "200/200 [==============================] - 0s - loss: 0.1149 - pearson_corr: 0.8806     \n",
      "Epoch 135/200\n",
      "200/200 [==============================] - 0s - loss: 0.1148 - pearson_corr: 0.8703     \n",
      "Epoch 136/200\n",
      "200/200 [==============================] - 0s - loss: 0.1148 - pearson_corr: 0.8829     \n",
      "Epoch 137/200\n",
      "200/200 [==============================] - 0s - loss: 0.1147 - pearson_corr: 0.8837     \n",
      "Epoch 138/200\n",
      "200/200 [==============================] - 0s - loss: 0.1147 - pearson_corr: 0.8780     \n",
      "Epoch 139/200\n",
      "200/200 [==============================] - 0s - loss: 0.1147 - pearson_corr: 0.8828     \n",
      "Epoch 140/200\n",
      "200/200 [==============================] - 0s - loss: 0.1146 - pearson_corr: 0.8813     \n",
      "Epoch 141/200\n",
      "200/200 [==============================] - 0s - loss: 0.1145 - pearson_corr: 0.8789     \n",
      "Epoch 142/200\n",
      "200/200 [==============================] - 0s - loss: 0.1145 - pearson_corr: 0.8808     \n",
      "Epoch 143/200\n",
      "200/200 [==============================] - 0s - loss: 0.1144 - pearson_corr: 0.8804     \n",
      "Epoch 144/200\n",
      "200/200 [==============================] - 0s - loss: 0.1144 - pearson_corr: 0.8837     \n",
      "Epoch 145/200\n",
      "200/200 [==============================] - 0s - loss: 0.1143 - pearson_corr: 0.8809     \n",
      "Epoch 146/200\n",
      "200/200 [==============================] - 0s - loss: 0.1143 - pearson_corr: 0.8847     \n",
      "Epoch 147/200\n",
      "200/200 [==============================] - 0s - loss: 0.1142 - pearson_corr: 0.8824     \n",
      "Epoch 148/200\n",
      "200/200 [==============================] - 0s - loss: 0.1142 - pearson_corr: 0.8800     \n",
      "Epoch 149/200\n",
      "200/200 [==============================] - 0s - loss: 0.1141 - pearson_corr: 0.8778     \n",
      "Epoch 150/200\n",
      "200/200 [==============================] - 0s - loss: 0.1140 - pearson_corr: 0.8782     \n",
      "Epoch 151/200\n",
      "200/200 [==============================] - 0s - loss: 0.1139 - pearson_corr: 0.8848     \n",
      "Epoch 152/200\n",
      "200/200 [==============================] - 0s - loss: 0.1138 - pearson_corr: 0.8830     \n",
      "Epoch 153/200\n",
      "200/200 [==============================] - 0s - loss: 0.1139 - pearson_corr: 0.8811     \n",
      "Epoch 154/200\n",
      "200/200 [==============================] - 0s - loss: 0.1139 - pearson_corr: 0.8844     \n",
      "Epoch 155/200\n",
      "200/200 [==============================] - 0s - loss: 0.1139 - pearson_corr: 0.8813     \n",
      "Epoch 156/200\n",
      "200/200 [==============================] - 0s - loss: 0.1139 - pearson_corr: 0.8841     \n",
      "Epoch 157/200\n",
      "200/200 [==============================] - 0s - loss: 0.1138 - pearson_corr: 0.8847     \n",
      "Epoch 158/200\n",
      "200/200 [==============================] - 0s - loss: 0.1138 - pearson_corr: 0.8839     \n",
      "Epoch 159/200\n",
      "200/200 [==============================] - 0s - loss: 0.1137 - pearson_corr: 0.8819     \n",
      "Epoch 160/200\n",
      "200/200 [==============================] - 0s - loss: 0.1137 - pearson_corr: 0.8810     \n",
      "Epoch 161/200\n",
      "200/200 [==============================] - 0s - loss: 0.1136 - pearson_corr: 0.8818     \n",
      "Epoch 162/200\n",
      "200/200 [==============================] - 0s - loss: 0.1135 - pearson_corr: 0.8854     \n",
      "Epoch 163/200\n",
      "200/200 [==============================] - 0s - loss: 0.1135 - pearson_corr: 0.8835     \n",
      "Epoch 164/200\n",
      "200/200 [==============================] - 0s - loss: 0.1134 - pearson_corr: 0.8821     \n",
      "Epoch 165/200\n",
      "200/200 [==============================] - 0s - loss: 0.1133 - pearson_corr: 0.8819     \n",
      "Epoch 166/200\n",
      "200/200 [==============================] - 0s - loss: 0.1133 - pearson_corr: 0.8838     \n",
      "Epoch 167/200\n",
      "200/200 [==============================] - 0s - loss: 0.1132 - pearson_corr: 0.8867     \n",
      "Epoch 168/200\n",
      "200/200 [==============================] - 0s - loss: 0.1131 - pearson_corr: 0.8861     \n",
      "Epoch 169/200\n",
      "200/200 [==============================] - 0s - loss: 0.1131 - pearson_corr: 0.8830     \n",
      "Epoch 170/200\n",
      "200/200 [==============================] - 0s - loss: 0.1131 - pearson_corr: 0.8823     \n",
      "Epoch 171/200\n",
      "200/200 [==============================] - 0s - loss: 0.1131 - pearson_corr: 0.8826     \n",
      "Epoch 172/200\n",
      "200/200 [==============================] - 0s - loss: 0.1130 - pearson_corr: 0.8877     \n",
      "Epoch 173/200\n",
      "200/200 [==============================] - 0s - loss: 0.1130 - pearson_corr: 0.8841     \n",
      "Epoch 174/200\n",
      "200/200 [==============================] - 0s - loss: 0.1129 - pearson_corr: 0.8756     \n",
      "Epoch 175/200\n",
      "200/200 [==============================] - 0s - loss: 0.1128 - pearson_corr: 0.8871     \n",
      "Epoch 176/200\n",
      "200/200 [==============================] - 0s - loss: 0.1128 - pearson_corr: 0.8850     \n",
      "Epoch 177/200\n",
      "200/200 [==============================] - 0s - loss: 0.1127 - pearson_corr: 0.8861     \n",
      "Epoch 178/200\n",
      "200/200 [==============================] - 0s - loss: 0.1127 - pearson_corr: 0.8856     \n",
      "Epoch 179/200\n",
      "200/200 [==============================] - 0s - loss: 0.1126 - pearson_corr: 0.8856     \n",
      "Epoch 180/200\n",
      "200/200 [==============================] - 0s - loss: 0.1127 - pearson_corr: 0.8853     \n",
      "Epoch 181/200\n",
      "200/200 [==============================] - 0s - loss: 0.1126 - pearson_corr: 0.8867     \n",
      "Epoch 182/200\n",
      "200/200 [==============================] - 0s - loss: 0.1126 - pearson_corr: 0.8879     \n",
      "Epoch 183/200\n",
      "200/200 [==============================] - 0s - loss: 0.1125 - pearson_corr: 0.8863     \n",
      "Epoch 184/200\n",
      "200/200 [==============================] - 0s - loss: 0.1124 - pearson_corr: 0.8855     \n",
      "Epoch 185/200\n",
      "200/200 [==============================] - 0s - loss: 0.1124 - pearson_corr: 0.8848     \n",
      "Epoch 186/200\n",
      "200/200 [==============================] - 0s - loss: 0.1123 - pearson_corr: 0.8866     \n",
      "Epoch 187/200\n",
      "200/200 [==============================] - 0s - loss: 0.1123 - pearson_corr: 0.8831     \n",
      "Epoch 188/200\n",
      "200/200 [==============================] - 0s - loss: 0.1122 - pearson_corr: 0.8874     \n",
      "Epoch 189/200\n",
      "200/200 [==============================] - 0s - loss: 0.1122 - pearson_corr: 0.8890     \n",
      "Epoch 190/200\n",
      "200/200 [==============================] - 0s - loss: 0.1122 - pearson_corr: 0.8904     \n",
      "Epoch 191/200\n",
      "200/200 [==============================] - 0s - loss: 0.1121 - pearson_corr: 0.8851     \n",
      "Epoch 192/200\n",
      "200/200 [==============================] - 0s - loss: 0.1122 - pearson_corr: 0.8882     \n",
      "Epoch 193/200\n",
      "200/200 [==============================] - 0s - loss: 0.1122 - pearson_corr: 0.8826     \n",
      "Epoch 194/200\n",
      "200/200 [==============================] - 0s - loss: 0.1121 - pearson_corr: 0.8895     \n",
      "Epoch 195/200\n",
      "200/200 [==============================] - 0s - loss: 0.1121 - pearson_corr: 0.8859     \n",
      "Epoch 196/200\n",
      "200/200 [==============================] - 0s - loss: 0.1120 - pearson_corr: 0.8838     \n",
      "Epoch 197/200\n",
      "200/200 [==============================] - 0s - loss: 0.1119 - pearson_corr: 0.8858     \n",
      "Epoch 198/200\n",
      "200/200 [==============================] - 0s - loss: 0.1119 - pearson_corr: 0.8858     \n",
      "Epoch 199/200\n",
      "200/200 [==============================] - 0s - loss: 0.1119 - pearson_corr: 0.8868     \n",
      "Epoch 200/200\n",
      "200/200 [==============================] - 0s - loss: 0.1118 - pearson_corr: 0.8852     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f73ddfb2510>"
      ]
     },
     "execution_count": 16,
=======
      "[1 1 3 6 4]\n",
      "[3 6]\n"
     ]
    }
   ],
   "source": [
    "print(np.convolve(x, y, mode='same'))\n",
    "print(np.convolve(x, y, mode='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 31,
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "model.fit(X, Y, epochs=200, batch_size=64)"
=======
    "x = np.arange(12).reshape(3, 4)\n",
    "x"
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 33,
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "((100,), (100, 1))"
      ]
     },
     "execution_count": 14,
=======
       "array([[ 0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  1,  2,  3,  0],\n",
       "       [ 0,  4,  5,  6,  7,  0],\n",
       "       [ 0,  8,  9, 10, 11,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 33,
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "x = np.linspace(0, np.pi*2., 100)\n",
    "y_true = np.sin(x)\n",
    "y_pred = model.predict(x)\n",
    "y_true.shape, y_pred.shape"
=======
    "np.pad(x, ((1, 1), (1, 1)), 'constant')"
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, BatchNormalization, Activation\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=784, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,510.0\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(100, input_dim=784))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,910.0\n",
      "Trainable params: 79,710.0\n",
      "Non-trainable params: 200.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
<<<<<<< HEAD
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhAAAAFkCAYAAABxWwLDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3Xm8zdX3x/HXds0UQqQ0UqZkrKREpqJMDV8aFGkwT6Uk\nitJgnjKkUUlpUpKvaPhWojI0D5qUSoqiIkr27491/UIu99x7ztlneD8fj/PAuedzznIMZ929117L\nee8RERERiUSe0AGIiIhI8lECISIiIhFTAiEiIiIRUwIhIiIiEVMCISIiIhFTAiEiIiIRUwIhIiIi\nEVMCISIiIhFTAiEiIiIRUwIhIiIiEYtpAuGcO80596xz7jvn3A7nXKtsXNPQObfcObfVObfKOXdp\nLGMUERGRyMV6BaII8A7QDdjv0A3n3JHAc8CLwAnAeOAe51zT2IUoIiIikXLxGqblnNsBtPHeP7uP\nx9wJnOW9r77LfbOAYt77FnEIU0RERLIh0WogTgYW7XHfAqBegFhEREQkC3lDB7CHssC6Pe5bBxzo\nnCvgvd+25wXOuZJAc2A1sDXmEYqIiKSOgsCRwALv/YZILky0BCInmgMzQwchIiKSxC4CHonkgkRL\nIH4AyuxxXxng172tPmRaDfDwww9TuXLlGIaWHPr27cvYsWNj/jrr18NLL8GiRbBiBXgPZctCxYpw\n3HFwzDFQtCgULGi3fPlg0yb48Uf44Qe7ffQRfPyxPd9hh8Epp0C7dvYcuRWv9yEZ6L0weh+M3od/\n6L2Ajz/+mIsvvhgyP0sjkWgJxBLgrD3ua5Z5f1a2AlSuXJlatWrFKq6kUaxYsZi+D4sXw+23w/PP\nQ0YGNG4M06dDq1ZQunTkz/fzz/C//8GLL8KTT8Ls2VCvHlx9NZx/PhQqlLM4Y/0+JBO9F0bvg9H7\n8A+9F7uJuAQg1n0gijjnTnDO1ci86+jMX5fP/PrtzrkHd7lkauZj7nTOHeec6wacB4yJZZyyb97D\nCy/A6afDqafC6tVw9922ivDf/8Lll+cseQA46CBo2xYmTYJvvoEnnoAiReDSS6F8eRg/Hv78M6q/\nHRERiYJYn8KoA6wElmN9IEYDK4ChmV8vC5Tf+WDv/WqgJdAE6x/RF7jce7/nyQyJk5UrbWuheXP4\n4w+YMwfeew+6dIGSJaP7WvnywbnnwsKF8Nlnllj06wdVqsDjj1siIyIiiSGmCYT3/n/e+zze+4w9\nbp0zv97Je3/GHte86r2v7b0v5L2v6L1/KJYxyt79/rt9eNepYz9/4QV4801o3RryxOHwb4UKtjXy\n3ntWU3HBBZbIvPde7F9bRET2L9H6QEgudejQIdfP8cwzULkyTJ0Kt91mRZJNm4JzUQgwQlWrwrx5\nViPx+++W0IwYAX//ve/rovE+pAq9F0bvg9H78A+9F7kTt06UseKcqwUsX758uYphcmnbNujbF6ZM\ngRYtrC7hqKNCR/WPbdtg8GAYNcpqMR58MLHiExFJNitWrKB27doAtb33KyK5VisQAsBXX9mH8r33\n2srDc88l3odzgQK2+vDKK1ZwWb06PPZY6KhERNKTEghh7lyoVQs2bIA33oCrrgqzXZFdDRpYLcQ5\n50D79jBkCOzYEToqEZH0ogQijXkPd9xhPRxOPx2WLwdbyUp8Bx4IM2daT4pbb7Uiy82bQ0clIpI+\nlECkqR07oE8fGDjQ6gqefhpKlAgdVWScg+uvh6eesn4Up50G334bOioRkfSgBCINbdsGHTpYkeSU\nKTBsWGJvWexPmzbWIXPDBjvq+fnnoSMSEUl9SiDSzK+/2gmLZ56xro9XXx06oug44QSr3yhc2Gok\nPvkkdEQiIqlNCUQa+e03OPNMq3V44QXr9JhKDj3U5mqULGk1He+/HzoiEZHUpQQiTWzZYqcWPvzQ\nJmg2aBA6otgoUwZeftmSiYYNrQmWiIhEnxKINLB1q602LFsG8+dbN8dUVqqUda6sUAGaNLGkSURE\noksJRIr780874vjqq9Yc6pRTQkcUHyVK2DbN4Yfbts2aNaEjEhFJLUogUtiOHTYWe8ECm6LZsGHo\niOKrWDFbccmbF5o1s1MaIiISHUogUtiNN1qr51mzbBx3OjrkEFuJ2LABWrZUsykRkWhRApGi7rvP\nujSOHAnt2oWOJqyKFeH55+GDD+D882H79tARiYgkPyUQKejFF22exVVXQb9+oaNJDHXqWLfNhQvh\nmmtCRyMikvyUQKSYjz6Cc8+Fxo2t02Qyd5iMtqZNYdw4GD8e7r8/dDQiIsktb+gAJHp+/hnOPhvK\nl4fZs614UHbXrRu8+6514KxUCerVCx2RiEhy0gpEitixAy65BDZtsvHcBx4YOqLE5JytzNSta7Uh\n330XOiIRkeSkBCJF3HabHVmcOROOPDJ0NIktf3548klboWnb1hptiYhIZJRApICFC2HIELudeWbo\naJJDmTLWG+P996Fv39DRiIgkHyUQSW7NGrjwQmuUNHhw6GiSS+3aVlQ5dSo8/njoaEREkkvqlNnN\nmwfr19u3lmXKQOnSkJEROqqY+vNP62tQuDA8/HDK/3Zj4sor7dhrly6WUBx9dOiIRESSQ+okEEOG\n7P5r5yyJ2JlQlC377x93/rxUKciTfIsxQ4bYaO7Fi+23IJFzDqZPh5o1oX17eP11q5EQEZF9S50E\nYvFiKFcO1q3b/fbDD/bj11/Dm2/az3/9dfdrMzLg4IP/SSrKlrUeyLv+eud9RYuG+f3t4ZVXYMQI\n6zZ54omho0luxYrBo49C/fpwww0walToiEREEl/qJBAFC9rxg+wcQdiyBX780ZKLnQnG2rX//PjR\nR7auvW4dbNu2+7VFilgisTPB2PnzPW8lS8asi9Mvv0DHjtCggboqRsuJJ8Idd9j7ecYZ0KJF6IhE\nRBJb6iQQkShcOHvJhvfWWGHtWrvtTDh2/nrtWhuwsHYtbNy4+7X588NRR1lLyObNoVEjOOCAXIfu\nPXTtaosoM2ao7iGa+va1vLFzZ/jwQ8sBRURk79Izgcgu56B4cbtVrrzvx27dasnF99/vnlzMnw+T\nJ1vTgfr14T//gQsuyPGn08MP24TNRx+Fww/P0VNIFvLkgXvugWrVoHt3e49FRGTvlEBEy762UL74\nAhYssBaRPXtC795w1llw8cVwzjl2bTasXm0fbJdcYnmIRF+5cnDXXXY0tl07y/VEROTfku/oQTI6\n5hgbwjB/vq1QjB5tqxUXXGD1EldfDUuX2v5EFryHyy+HEiWsFbPETvv2cN559kf2ww+hoxERSUxK\nIOLt4INtFeLNN+GTT+xTat48m+pUubJV8q1d+6/L7rkHXnrJftSci9hyznadMjJsJPo+8joRkbSl\nBCKk446D4cNtb2LhQqhTB4YOtXGarVvDs8/C9u18+62dDujc2UZSS+yVLg3TptkfwYMPho5GRCTx\nKIFIBBkZ0KSJVUiuXQsTJsC330Lr1vjDD+f1RoM5tsDXjB4dOtD00qaNHZft21dbGSIie1ICkWiK\nF7dtjeXLYcUKPqvShhafj+et9UdR/KKW9i3x33+HjjJtjBljB2j69AkdiYhIYlECkcDWlatJvZWT\n6X3e97i777bmV61b28CG22+Hn34KHWLKK1nSBm499piVqoiIiFECkcD69LHeBCMmF7VpT2+/bbfG\njWHYMDjsMDvT+fbboUNNaTunnXbrBr//HjoaEZHEoAQiQS1aZI2Mxoyxgr7/V6cO3Hef1UgMH24z\nQE48EU45xb5N/uuvYDGnKudgyhRb8NlzZpuISLpSApGAtm2zhlENGlivqb0qWdKOZnz2GcyZY82o\n2re39tl33GEDMyRqjj4abr4Zxo+38hQRkXSnBCIBjRwJX35pvQj2O48rI8PqIl56Cd591+Zu3HST\nHQXt1cueSKKib184/ni44grYvj10NCIiYSmBSDBffmk7E/37Q9WqEV5cvTrcey988w306wePPAIV\nK8L558OyZTGJN53ky2e9IVauhLvvDh2NiEhYSiASiPfWpLJ0aRg8OBdPVKaMFVl+840Ndli5EurW\nteLLF15Qa8VcOOkka+h1442wfn3oaEREwlECkUDmzIHnn7c+UkWKROEJCxe2ORuffgqzZ9to8ubN\noVYteOIJ2LEjCi+Sfm6/3d66QYNCRyIiEo4SiASxZYsd22zZ0koaoiojw7Yx3n7bjneULGm/rloV\nHnpIG/oROvhguOUWmD5dBZUikr6UQCSIUaOsi/W4cdkonMwp52wbY9EiWLIEKlSwXs3HHmtTunQE\nNNu6doVq1aBHDy3kiEh6UgKRANassZOXffrYZ3pcnHwyzJ0L77wDtWvb0YKKFe3b6j//jFMQyStv\nXpg40aawP/RQ6GhEROJPCUQCuP56OOAAK8yLuxNOgMcfh/fftwrBq66yFYlHHlGx5X6cfjp06AAD\nBlh5iYhIOlECEdiSJfZZPXw4HHhgwECqVbNOlu+/b0WWF10Ep51mJzgkSyNHWnvr228PHYmISHwp\ngQhoxw7o3Rtq1oROnUJHk6lqVXjqKauT2LjRtjeuukpnFrNw6KFw7bUwdiysXh06GhGR+FECEdBD\nD9nBiPHj7aBEQmnc2FYfxo2zI6DHHWczOFQx+C/XXmsHWwYODB2JiEj8KIEIZPNm+8A5/3zbKUhI\n+fJZO+xPP7XzpZdfbhv/H34YOrKEUqSIbUE9+qgVVYqIpAMlEIGMGQMbNsCdd4aOJBsOPhhmzLB5\nGz/9BDVqWPbzxx+hI0sYHTtaPWq/fqo9FZH0oAQigB9/hBEjrIfAUUeFjiYCjRrZwK4hQywDql4d\nXn45dFQJISMDRo+2otgnnggdjYhI7CmBCGDYMPvAueGG0JHkQIECNqjj3XfhkEPgjDOsh4TGh9O4\nMZx9Nlx3nY1kFxFJZUog4uyzz2yi4w03WOFd0qpUCV55BaZOtSLLKlXgmWdCRxXcyJE2w2zSpNCR\niIjElhKIOLvhBvvGvWfP0JFEQZ48dsTzo4+gTh1o08b6R2zYEDqyYCpVgi5d4Lbb1FxKRFKbEog4\nWrrU9sdvuQUKFQodTRQdeig8+6ydS50/31YjnnoqdFTBDBliw9FGjQodiYhI7CiBiBPvreVx9epw\n8cWho4kB5+w39uGHNmfj3HPt12lYG1GunDUIGzMG1q0LHY2ISGwogYiTefPgtdfs2GbCNY2KpkMO\ngTlz7Njnc8/B8cfDwoWho4q7666D/Pnh1ltDRyIiEhtKIOJgxw4YNMh6MDVvHjqaOHAOLrnE5mpU\nrgzNmtmZ1c2bQ0cWNyVKWBIxbRp8+WXoaEREok8JRBw8/ji89551K3QudDRxVL48LFhgRxLuu8/m\naqxYETqquOnVC0qVspoIEZFUowQixrZvtw+Qs86C+vVDRxNAnjzQvbslDoULW33EiBHw99+hI4u5\nwoXhppts2uq774aORkQkupRAxNiMGbBqlfbCqVTJjqH07QvXXw9NmsCaNaGjirnOneGYY+DGG0NH\nIiISXUogYmjbNhg6FM47D2rVCh1NAsif36pIX3wRPv/chkekePOpfPng5putnvStt0JHIyISPUog\nYmj6dPj2W2tdLbvYOVOjQQNrPtWzJ2zdGjqqmGnf3mpJVQshIqlECUSMbN5s2xYXX2wfHrKHgw6C\np5+2Asu777baiE8/DR1VTGRk2CrEggWweHHoaEREokMJRIxMnmwdnW+6KXQkCcw5K7B8801bgahd\n2yoOU9B551lLDP19EJFUoQQiBjZvtoMGnTvD0UeHjiYJ1KgBy5b9M0vj6qtTbksjTx6rh3nxRfjf\n/0JHIyKSe3FJIJxz3Z1zXznn/nDOLXXO1d3HY093zu3Y4/a3c+7geMQaDZMnw8aNSTquO5SiRW2W\nxt13wwMPQL16VmiZQtq0gZo1rRbC+9DRiIjkTswTCOfcf4DRwE1ATeBdYIFzrtQ+LvNARaBs5u0Q\n7/2PsY41GnZdfTjiiNDRJBnn4Ior7Ljn77/blsacOaGjihrnrKD21VfhpZdCRyMikjvxWIHoC0zz\n3s/w3n8CXA1sATrv57qfvPc/7rzFPMoo0epDFNSoAcuXW6+Itm2tb8T27aGjioqWLeHEE2HwYK1C\niEhyi2kC4ZzLB9QGXtx5n/feA4uAevu6FHjHOfe9c+4F59wpsYwzWjZvhpEjtfoQFQceaLPPR4yw\nN7V5c/gxafLILDlnJzKWLNEqhIgkt1ivQJQCMoA9hxqvw7Ym9mYtcBVwLtAOWAO84pyrEasgo2XK\nFJterdWHKHEOrr0WFi2CDz6wLY0U6MZ05plQp476g4hIcssbOoA9ee9XAat2uWupc+4YbCvk0qyu\n69u3L8WKFdvtvg4dOtChQ4eYxLkn1T7EUKNGNkvj3HOt+dSUKdCpU+iocsw528Jo3dpOZJx+euiI\nRCQdzJo1i1mzZu1236ZNm3L8fM7HcCM2cwtjC3Cu9/7ZXe5/ACjmvW+bzecZAdT33v9rHJVzrhaw\nfPny5dQK2C961CgYONAODiiBiJFt22ws+D332I9jxliv6CTkvZ3IKFXKFlhEREJYsWIFtWvXBqjt\nvY9oXHJMtzC8938By4HGO+9zzrnMX78RwVPVwLY2EtIff1gCcdllSh5iqkABO+Y5ZQpMnWpFlkla\nF7FzFeLFF9WdUkSSUzxOYYwBrnDOdXTOVQKmAoWBBwCcc7c75x7c+WDnXG/nXCvn3DHOuarOuXFA\nI2BSHGLNkXvvhZ9+ssMCEmPOWaOpl1+21td168I774SOKkfatoWqVeGWW0JHIiISuZgnEN772cA1\nwDBgJVAdaO69/ynzIWWB8rtckh/rG/Ee8ApwPNDYe/9KrGPNiT//tAGTF15oY5slTk49Fd5+2/YA\n6teHp54KHVHE8uSxMd8LFqREbaiIpJm4dKL03k/23h/pvS/kva/nvV+2y9c6ee/P2OXXI733Fb33\nRbz3pb33jb33r8Yjzpx46CGbuDlwYOhI0lD58vDaa3D22VZgOWxY0jVXOP98OO44rUKISPLRLIxc\n2L4dbr8d2rWDKlVCR5OmCheGRx+1T+CbbrLZ2X/8ETqqbMvIgEGD4LnnYOXK0NGIiGSfEohcmD0b\nvvjCPgAkIOdsL+DJJ2HuXGjYEH74IXRU2dahAxx5JNxxR+hIRESyTwlEDu3YAcOHw1lnQcDTo7Kr\ndu1sS2PNGjjpJHjvvdARZUvevDBgADz+OKxatf/Hi4gkAiUQOfTMM/DRR/aNrySQnd0qS5a04sp5\n80JHlC2dOsHBB1szMhGRZKAEIge8t9WHhg3hlKSY0pFmDjvMRl42bgytWtmEswRXsCD07w8zZlhR\nrohIolMCkQMvvmjDIjXzIoEVLWo1Eb17Q/fucM01tu+UwK6+GooUgdGjQ0ciIrJ/SiBy4I47rO6h\nSZPQkcg+ZWRYu+sJE2DsWLjggoQ+oXHAAdCzpzXbXL8+dDQiIvumBCJCy5bZCsT111vxvySBnj3h\n6adh/nw44wxrG5qgevWyHydMCBuHiMj+KIGI0J13QoUKVvAvSaRVKxt9+eWXVrjyxRehI9qrUqXg\nyith4kT47bfQ0YiIZE0JRARWrbJt9QEDbHVckkydOrBkiS0dnXKKtcJOQP3723j4adNCRyIikjUl\nEBEYORLKloWOHUNHIjl29NHwxhtw1FF2jOb550NH9C+HHQYXXQTjxtmsFRGRRKQEIpu+/96O2PXp\nY1OlJYmVKgUvvWRVsK1awf33h47oX669Fr77Dh55JHQkIiJ7pwQim8aNg0KF7KidpIDChW0/6vLL\noXNnO1qTQIO4qlSBc86xVa8EP30qImlKCUQ2bNwIU6dCt25w4IGho5GoyZvX/mCHDLFxqn37JtSn\n9YAB1u00AXdZRESUQGTHtGmwbds/R+wkhTgHQ4fCXXfZ2cmLL06YwoP69aFePbW3FpHEpARiP7Zt\ng/HjrXCybNnQ0UjMdOtm41WffNL2DjZvDh0RztkqxGuv2eEREZFEogRiPx55BNautaN1kuLOO8+a\nTb3xBjRtCr/8EjoiWrWCY4+1WggRkUSiBGIfduyAUaPsP/FKlUJHI3Fxxhl2QmPVKjj9dMseA8qT\nx05kzJkDn34aNBQRkd0ogdiH//7XitiuuSZ0JBJXdevavsHPP8Opp1r3yoAuuQTKlNGQLRFJLEog\n9mHkSDjpJPsMkTRTuTIsXmwtR089FT78MFgoBQpYAe+MGfDjj8HCEBHZjRKILCxbBq+8YsvHGpqV\npo44wlYiDj7YtjOWLw8WylVXWS5z113BQhAR2Y0SiCyMHAnHHANt2oSORIIqUwZefhkqVoRGjSyh\nCOCgg6zn1eTJsGVLkBBERHajBGIvvvoKnngC+vXT0CwBSpSAhQutNqJ5cyuOCaBPHyvLmDEjyMuL\niOxGCcRejBtnnxmXXRY6EkkYRYvCvHn/zM94+um4h3D00TZGfsyYhGqYKSJpSgnEHjZuhHvvha5d\nbVyCyP8rWNAaTbVtC+efD7NmxT2E/v3hs89g7ty4v7SIyG6UQOxh+nT46y/o3j10JJKQ8uWz7mIX\nX2wzt++7L64vf/LJ1uJ61Ki4vqyIyL/kDR1AIvnrLxuHcNFFalst+5CRYYlDoUJW2fjHH3HNOPv3\nt62MN9+0Y8YiIiEogdjF44/Dt9/aUEaRfcqTx45EFCoEPXrA1q1x63feqhVUqGCNpWbPjstLioj8\nixKITN7bf8jNmsHxx4eORpKCc/aXpmBBa1f6119w/fUxf9mMDDuR0asXfP21tasQEYk31UBkevVV\nWLHCjm6KZJtzMHw43HQTDBwIw4bF5WUvuwyKFYOJE+PyciIi/6IEItOYMVC1qq1AiETEObj5Zrj1\nVkskBg+2Ja0YKlIErrzSin5/+y2mLyUisldKILDBi3Pn2uqD2lZLjg0aBCNGWCIxcGDMk4gePawr\nZZwPgoiIAKqBAKxxVOnScOGFoSORpHfttZA3r2Wj3sMdd8QsKz3sMLjgAhg/3pIJdU0VkXhK+xWI\nn3+GBx+Ebt2sFk4k1/r2tax0xAhLKGK4EtG3r7Vef+aZmL2EiMhepf0KxPTpsH27dZ4UiZreve2o\nZ69e1nd69OiYrETUqQOnnQZjx1pvCBGReEnrBOKvv2DSJGscdfDBoaORlNOzpyURPXpYEjF2bEyS\niL59LXlYtswSChGReEjrBOLJJ61xVJ8+oSORlNW9uyUR3bpZ8jBmTNSTiFatbNDW2LEwc2ZUn1pE\nJEtpXQMxbhyccQZUrx46EklpXbvCXXfZX7j+/aNeE5GRYTsms2dbQiwiEg9pm0AsWWKzBNS2WuKi\nWzfbLxs7NiaFlZ062fTYyZOj+rQiIllK2wRi3DioWBFatAgdiaSN7t2tdeTo0TBgQFSTiAMOsLle\nd99tvSFERGItLROIb76x+oedhfIicdOjh418HTUq6s2mevSwY8mqgxCReEjLIspJk+w7tksvDR2J\npKWePe3scL9+kC8f3HJLVJ726KOhdWtbXevSRV1VRSS20i6B2LzZej906QJFi4aORtJW376WRAwY\nYEnEkCFRedo+faBhQ1i0CJo2jcpTiojsVdolEDNmwK+/2nKvSFDXXmvNSAYNsqMUgwbl+ikbNIAa\nNWwVQgmEiMRSWiUQO3bY9nPbtnDEEaGjEQFuuMFWIm68EfLnt6QiF5yz2p5OnWxI3LHHRilOEZE9\npFUJ4cKF8Mkn9h+sSMIYMsRWHwYMsMlYudS+vXVWnTAhCrGJiGQhrRKI8eOhZk049dTQkYjs4ZZb\nbPWhTx+YOjVXT1WwoPWuuv9++OWXKMUnIrKHtEkgPv0U5s+31QdVp0vCcQ7uvNP+gnbtCvfdl6un\nu/pqK6/I5dOIiGQpbRKICRNsWbd9+9CRiGTBOetU2bWrHRN6+OEcP1XZsvZ3fdIk+PvvKMYoIpIp\nLRKIjRvhwQftu7ICBUJHI7IPztmnfqdO1qjkiSdy/FS9esHq1TB3bvTCExHZKS0SiHvvhT//tG/s\nRBJenjzWk7p9e+jQIccZQJ06cMopUanLFBH5l5RPIP7+276ha9/elnVFkkJGhi2btWoF550HL7yQ\no6fp1QteeQXeey+64YmIpHwCMXeuLeP26hU6EpEI5c0Ls2ZZR6jWrS0TiFC7dnDooTrSKSLRl/IJ\nxIQJtoxbp07oSERyIH9+q4M49VQ4+2ybQx+BfPlsCOjMmbB+fYxiFJG0lNIJxPvvw8sva/VBklzB\ngjBnDtSqBWedBStWRHT5FVfYj9OnxyA2EUlbKZ1ATJhgy7ft2oWORCSXihSB556D446DZs3gww+z\nfWmpUnDRRXDXXdYbQkQkGlI2gdiwwY7Rd+tmy7giSe/AA+G//4XDDoPGjW3YRTb16gXffQdPPRXD\n+EQkraRsAnHPPeD9P8u3IimhRAkb6nLQQZZErF6drcuqV7cx3xMnxjQ6EUkjKZlAbN9uy7UXXgil\nS4eORiTKSpeGRYuswLJJE/j++2xd1qsXLF4My5fHOD4RSQspmUDMmQNr1qh4UlJYuXLw4ouwbZsd\n8/zpp/1ecs45NsZeqxAiEg0pmUBMmAANGkCNGqEjEYmhI4+0JGLDBmje3Hq270PevHakc9Ys+PHH\n+IQoIqkr5RKIlSvhtdegZ8/QkYjEwbHHWk3E119Dixbw++/7fPjll1uTy7vvjlN8IpKyUi6BmDgR\nypeHNm1CRyISJ8cfDwsWwAcfWOvrP/7I8qEHHQSXXAKTJ+tIp4jkTkolED/9BI88Ykc38+YNHY1I\nHNWpA/PmwdKlcP75Nj0uCz17wtq18OSTcYxPRFJOSiUQ06fbNOQuXUJHIhLAaadZBfHChXDxxTZJ\nbi+qVYMzztB8DBHJnZRJILZvhylTrONeqVKhoxEJpFkzeOwx6xjVpQvs2LHXh/XsaWM13n47zvGJ\nSMqISwLhnOvunPvKOfeHc26pc67ufh7f0Dm33Dm31Tm3yjl36f5e4+WX4dtvVTwpQps2MGOGjQPv\n3ds6qu3hnHPsEIeOdIpITsU8gXDO/QcYDdwE1ATeBRY45/a6TuCcOxJ4DngROAEYD9zjnGu6r9d5\n9FE7unnAjtHsAAAgAElEQVTCCdGLXSRpXXghTJ0KkybBoEH/+nJGhh3pfOwxWLcuQHwikvTisQLR\nF5jmvZ/hvf8EuBrYAnTO4vFdgS+99wO895967+8Cnsh8niy9844aR4ns5sorYfRouP12u+3h8sut\n2FhHOkUkJ2KaQDjn8gG1sdUEALz3HlgE1MvispMzv76rBft4PABlykDr1jmPVSQl9esHN98MN9xg\nqxG7KFHCjnROmbLPQxsiInsV6xWIUkAGsOci6TqgbBbXlM3i8Qc65wpk9UIXXKCjmyJ7NWQI9O9v\nBUIPPLDbl3Ye6dSUTpGcW7gQtmwJHUX8pcxH7rJlfWnVqthu93Xo0IEOHToEikgkQTgHI0dal8rL\nL4ciRaxXBFC16j9HOtu3DxynSBJaswbOOsv+DXXrFjqafZs1axazZs3a7b5Nmzbl+PlinUCsB/4G\nyuxxfxnghyyu+SGLx//qvd+W1QtNmjSWWrVq5TROkdTmnLWf/P13K7AsXBhatgSsdqhNGzvSWXef\n56NEZE9TplhOfskloSPZv719U71ixQpq166do+eL6RaG9/4vYDnQeOd9zjmX+es3srhsya6Pz9Qs\n834Ryak8eWwL4+yz4dxz7ewz9ksd6RSJ3B9/WBFyp05wwAGho4m/eJzCGANc4Zzr6JyrBEwFCgMP\nADjnbnfOPbjL46cCRzvn7nTOHeec6wacl/k8IpIbefPamefTT7dmEEuXkpEBPXroSKdIpB59FH7+\n2f79pKOYJxDe+9nANcAwYCVQHWjuvf8p8yFlgfK7PH410BJoAryDHd+83Hu/58kMEcmJAgXg6aeh\nZk3bvH3nHTp31pFOkUh4b3UPLVpAhQqhowkjLp0ovfeTvfdHeu8Lee/ree+X7fK1Tt77M/Z4/Kve\n+9qZj6/ovX8oHnGKpI3CheG55+CYY6BZM0qs+0RHOkUi8Prr6j+UMrMwRCRCxYrZGPCDD4YmTejX\n9itN6RTJpokToVIlaLrPHsmpTQmESDorWdIOsRcqxLFdG3NB/e80pVNkP9assd4pPXrYAad0pQRC\nJN0dcggsWgTbt3PP1034YumPmtIpsg87j2527Bg6krCUQIgIHHEELFpE0b9+4eX8zbl31C+hIxJJ\nSDuPbnbunJ5HN3elBEJEzLHH4hYu5Oi833DZ4y1Y9/lvoSMSSTizZtnRze7dQ0cSnhIIEfnH8cez\n/bkFVPEf8kfTVvbtlogA/xzdbNkyfY9u7koJhIjs5oBGdZje5nnKrH6THW3P1blOkUyvvQbvvpve\nRzd3pQRCRP7lrOGn0opn8C++aLMztm8PHZJIcBMmQOXK0KRJ6EgSgxIIEfmXKlXANW3KgKOegGee\nsWb/O3aEDkskmK+/tgauvXql99HNXSmBEJG96tULxnx2Dp/dPBMeeQS6drVNYJE0NHmynbpIhqmb\n8aIEQkT2qkUL63R904cXwH332dm1vn2VREja2bIFpk+HLl2s/4MYJRAisld58kDPnvD44/B900vt\nW7Dx4+HGG0OHJhJXM2fCxo06urknJRAikqXLLoOCBa3zHl27wqhRcNttcOutoUMTiQvvLW9u1QqO\nOip0NIlFCYSIZKlYMaufnDYNtm4F+veHW26BwYNh9OjQ4YnE3Msvw4cf6ujm3iiBEJF96tEDfvrJ\nOvABMGgQDBwI11xj2xoiKWz8eKhWDRo1Ch1J4skbOgARSWzHHmsFlRMm2JaGcw6GD7culd272x5H\n586hwxSJui++gLlzrX5YRzf/TQmEiOxXr15w5pnWia9BA+x/0zFjbF+jSxcoUAAuuih0mCJRNWkS\nHHSQ/mpnRQmEiOxXs2ZQqZIt5zZokHmnc3DXXbBtm801zp8fzj8/aJwi0fLrr3DvvbaFV6hQ6GgS\nkxIIEdkv52wVokcPWL0ajjwy8wt58tgB+T//tJbX+fND69YBIxWJjgcesP4P3bqFjiRxqYhSRLLl\nkkvgwANt0WE3GRn2v23btrYCMX9+iPBEombHDpg40f46H3ZY6GgSlxIIEcmWokXhiitsweH33/f4\nYt681m2nRQtLJBYuDBKjSDQ8/zx8/jn07h06ksSmBEJEsq17d/jtN5gxYy9fzJcPHnsMGje2rjsv\nvxz3+ESiYfx4OPFEOPnk0JEkNiUQIpJtRxwB7drZf7B7Hc5ZoAA8+aRVWp59Nrz6atxjFMmNDz6A\nRYu0+pAdSiBEJCJ9+sCqVfDf/2bxgIIFYc4cqFfPtjQWL45rfCK5MWECHHIInHde6EgSnxIIEYnI\nKadAnTowbtw+HlSoEDz7LNStC2edBUuWxC0+kZxavx4eesi26vLnDx1N4lMCISIRcc5WIRYutBkB\nWSpc2Nr41ahhXajefDNuMYrkxLRp9uNVV4WNI1kogRCRiJ1/vi3zTpiwnwcWLWol7dWrWzeqt96K\nS3wikfrzTzuifMklUKpU6GiSgxIIEYlY/vzWYGfGDNiwYT8P3plEVKtmScSyZXGJUSQSjz8Oa9fa\n6ppkjxIIEcmRq64C723Q0H4dcIA1mKpcGZo2VRIhCcV7GDvW8tsqVUJHkzyUQIhIjpQubcu9kybZ\n8u9+HXigHd2oVElJhCSUxYth+XKtPkRKCYSI5FifPvD99zB7djYvKFYMFiywlYgmTeDtt2Man0h2\njB1reW3z5qEjSS5KIEQkx6pWtWXfsWNtGThbdq5EVK1qKxEqrJSAvvrK2pb07m2z4ST79HaJSK70\n6wcrVsBrr0Vw0Z5JhI54SiCTJtnCWMeOoSNJPkogRCRXdhaejRkT4YUHHGBJRPXqlkS88UZM4hPJ\nyq+/wj33WEFw4cKho0k+SiBEJFecg759rfHk559HePHO0xm1atkGdETLGCK5c++9sGUL9OgROpLk\npARCRHLtoougZMlsNJbam6JFYd48G3945pma4ilxsX27DYXr0AEOPTR0NMlJCYSI5FqhQtC1K9x3\nH2zcmIMnKFLE2l7Xrw8tW1qfbJEYeuop+PprWz2TnFECISJR0a0b/PUXTJ+ewycoXNj2QRo2hHPO\nsVUJkRjwHkaPhkaNoGbN0NEkLyUQIhIVZcvaVsaECZZI5EjBgvD007aV0bat/VwkypYssdPD/fuH\njiS5KYEQkajp1w++/TaCxlJ7U6CADSZo186mds2aFbX4RMBWH447zibNS84pgRCRqKlWzQ5TjB4d\nQWOpvcmXD2bOtCWNiy6C+++PWoyS3r74wha2+vVT46jc0tsnIlF1zTWwcmUUDlNkZFjicOWV0Lmz\ndfwRyaXx4+Ggg2yOi+RO3tABiEhqadzYekONHg1nnJHLJ8uTB6ZMsVMaPXvC77/D9ddHJU5JP7/8\nYieF+vWzk0OSO0ogRCSqnLNViI4d4cMPrVt1rp9w1ChrOjVwIPz2G9x6q90vEoGpU63/Q/fuoSNJ\nDdrCEJGo+89/oFy5HLS3zopzcPPNMHIk3HabjQHdsSNKTy7pYNs2OyHUsSOUKRM6mtSgBEJEoi5/\nfptu+PDD8MMPUXzia66xLY2JE60uYvv2KD65pLKHH4Z163R0M5qUQIhITFx5pSUSUa99vPpqO6Ex\nc6Yd89y6NcovIKlmxw6ryWnVyo5vSnQogRCRmCheHLp0gcmTrfYxqjp0gDlzbJpny5ZWFyGSheef\nh48/hmuvDR1JalECISIx07evjUy+774YPHnLlrBgASxbBk2awPr1MXgRSQUjR8LJJ8Mpp4SOJLUo\ngRCRmDn8cFssGD06F+2t96VBA2s48dVXcNpp8M03MXgRSWZvvQWvvmqrDzq4E11KIEQkpq691j7X\nc9Xeel9q1YLFi60Won59W6sWyTRyJFSoAK1bh44k9SiBEJGYql7dZmONGJHL9tb7UrGiJRHFi8Op\np8LSpTF6IUkmX3xhY7v797fGphJdSiBEJOYGDID33oMXXojhi5QrZ2vVlStbO8z582P4YpIMRo+G\nkiWt94NEnxIIEYm5hg2hbl1bhYipEiUsS2ncGM45Bx54IMYvKIlq3Tor3u3dGwoXDh1NalICISIx\n55ytQrz0kh2aiKnChW3dunNn6NQJhg+P4d6JJKoJE2yoa7duoSNJXUogRCQu2raFY46JwyoEQN68\nMG0aDB0KN94IPXrA33/H4YUlEfz6K9x1F1x1lS1KSWwogRCRuMjIsE7UTz4Jn38ehxd0DoYMgenT\nbYrS+efDli1xeGEJbdo0+6Pu2zd0JKlNCYSIxM1ll0Hp0nFahdipSxd49llrOnXGGfDTT3F8cYm3\nbdtg7FgrnDz00NDRpDYlECISNwULQr9+Vtv43XdxfOGWLeF//4PVq6FePVi1Ko4vLvH00EM2wE1t\nq2NPCYSIxNXVV0ORIlEc9Z1ddepYf4h8+ayn8eLFcQ5AYu3vv211q107Dc2KByUQIhJXBx5oNY3T\npsGGDXF+8SOPhDfegKpV7ajnrFlxDkBi6emn4bPP4LrrQkeSHpRAiEjc9eplI5YnTgzw4jt7RVxw\nAVx4Idxyi455pgDv4bbbrMylbt3Q0aQHJRAiEnelS8OVV9pZ/SCTuAsUgAcftORhyBCruNu2LUAg\nEi3z58PKlXZqV+JDCYSIBNG/P/z+O9x9d6AAnLNPm1mz4PHHbSS4TmgkJe/h1lutPrZhw9DRpA8l\nECISRPnycMklNq8g6Df/7dvbSPBVq+DEE+GDDwIGIznxyiuwZInlgxrZHT9KIEQkmOuusyN3998f\nOJB69eCtt6zCs149eO65wAFJJG69FWrWhLPOCh1JelECISLBHHss/Oc/cPvt8OefgYM54gg72tmk\nCbRqBaNGqbgyCSxZYjNWBg3S6kO8xSyBcM6VcM7NdM5tcs794py7xzlXZD/X3O+c27HH7flYxSgi\n4Q0aBN98Yw2Agita1HptX3+9dSK69FL444/QUck+DB9uE9zbtg0dSfqJ5QrEI0BloDHQEmgATMvG\ndfOBMkDZzFuHWAUoIuFVqwbnnmtH8LZvDx0NkCePBTNzphVXNmgA334bOirZi5UrYd48uOEG+2OT\n+IrJW+6cqwQ0By733i/z3r8B9ATaO+fK7ufybd77n7z3P2beNsUiRhFJHDfeCF9+CY88EjqSXVx4\noW1prFtnXSzVuTLh3HYbHH201cFK/MUqZ6sH/OK9X7nLfYsAD5y0n2sbOufWOec+cc5Nds4dFKMY\nRSRB1KhhZQfDhyfY1O1atWDZMuuL3KiRTfVUXURC+OADeOIJGDjQprdL/MUqgSgL/LjrHd77v4Gf\nM7+WlflAR+AMYABwOvC8cyqNEUl1gwfbScrZs0NHsoeDD4ZFi6zzVdeu0Lmz6iISwLBh1pm8Y8fQ\nkaSviPI259ztwL66jHus7iFHvPe7/tfxoXPufeALoCHw8r6u7du3L8WKFdvtvg4dOtChg0ooRJJB\nnTp2DO/WW+1kRkLtaefLB5MmwUknWSLx3ntWbHnkkaEjS0sffGDlKdOnQ/78oaNJHrNmzWLWHvNf\nNm3KeZWA8xEsxznnSgIl9/OwL4FLgFHe+/9/rHMuA9gKnOe9fyaC1/wRGOS9n57F12sBy5cvX06t\nWrWy+7QikoCWLLFBmbNnw/nnh44mC++8Y+MeN22yLpbNmoWOKO1ccAG8/TZ8+qkSiNxasWIFtWvX\nBqjtvV8RybUR5fje+w3e+1X7uW0HlgDFnXM1d7m8MeCAN7P7es65w7CEZW0kcYpIcqpXD5o2haFD\nbdhWQqpRw+oiTjwRzjwTbr45wQo3UtvO1YdBg5Q8hBaTRULv/SfAAmC6c66uc64+MBGY5b3/Yefj\nMgslW2f+vIhzboRz7iTn3BHOucbAHGBV5nOJSBoYNgw+/NA+JBLWQQfZ+cGhQy3gs87SHI04Ue1D\n4ojlLuOFwCfY6YvngFeBq/Z4TEVgZ+HC30B14BngU2A68DbQwHv/VwzjFJEEcvLJ0KJFEnxjnyeP\nVX6+8IJta9SsCW+8ETqqlKbVh8QSswTCe7/Re3+x976Y976E9/4K7/2WPR6T4b2fkfnzrd77M733\nZb33Bb33R3vvu3rvldaLpJmhQ+GTT6zEIOE1aWIdjY46yppOjRiRwPsvyU2rD4klkeqcRUQAO5HR\nqpUlEgnRnXJ/Dj3UBjIMGGATwlq0gB9/3P91km3vvqvVh0SjBEJEEtLQofD55wkyIyM78uWz1ogL\nFtiKRI0allRIVAweDBUq2HgSSQxKIEQkIdWoYTMyhg1LgEmdkWjWzGoiKle27Y2rr4affw4dVVJb\nuhTmzrWkMl++0NHITkogRCRh3XwzfP01PPBA6EgidMghVlw5bpwVchx3HNx/v2ojcmjQIBu6ppkX\niUUJhIgkrJ0fGsOGJWH36IwM6NXLqkGbN7cW2A0awIsvap5GBF56yW633JJg3UlFCYSIJLZhw2wg\n5l13hY4khw45BB5+2D4Ff/vNtjWqVYMpU+D330NHl9C8t9WHunWhdevQ0cielECISEKrUAG6dLH6\nxI0bQ0eTC40aWW3ESy/ZlkaPHnDYYdC3L3z2WejoEtK8eVb/MHw4aKRi4lECISIJb/Bg2LoVRo0K\nHUkuOWeJxFNPwZdfWoHlQw/BscfaNsezzyZ496z42bHDVh9OP90WbSTxKIEQkYRXrhz06QNjx8La\nVJmMc8QRcMcd8O238OCDtrzSujUcfbR9y/3DD/t/jhT26KM29FSrD4lLCYSIJIUBA6BAARv3nVIK\nFrTWim++CW+9BY0b26dm+fI21/yVV9Ku6HLbNlt9aN0a6tcPHY1kRQmEiCSF4sXh+uvh7rvhiy9C\nRxMjdevCfffBd9/Zfs1779mWR6VKMHo0rF8fOsK4mDwZ1qyxBRpJXEogRCRp9OgBBx9sNREprUQJ\n6N0bPvoIXn4ZateGG26wltkXXWT3peiqxMaNtsp0+eWWN0niUgIhIkmjcGFrLjVrFixbFjqaOHAO\nGjaERx6xVYnbbrPf+BlnWOHlnXemXK3EHXdYwezNN4eORPZHCYSIJJVOnaBqVejfP2W/Cd+7UqXs\nN/3JJ/Dqq1Cvnn3Kli8PbdpYr+ekmDyWtTVrYPx4+20eckjoaGR/lECISFLJm9fKA159FebMCR1N\nAM7BaafBjBl2JGXcOPvkbdXKkonrrrMkIwkNGQIHHADXXhs6EskOJRAiknTOPNNmVg0YkGSDtqKt\neHHo3h2WL7cJoOefD9On2yCvevVg2rSk6b71/vt2mvXmmy2JkMSnBEJEktKoUdaLacqU0JEkiBo1\nYMIEW5WYPRsOOgi6dYOyZW2gyPz5CbvF4b015KxYEa64InQ0kl1KIEQkKR1/vFXqDxsGv/wSOpoE\nUqCArUTMm2dNqoYNs2/vW7Sw1tn9+1tL7QQqIHn2WZsxNmaMxnUnEyUQIpK0hg2zLYyUay4VLYcc\nYvs8H3xg2xzt21vr7Jo1oXp1O8WxZk3QELdts5ymeXPLcSR5KIEQkaRVtqzVDE6cqHlU++Qc1Kpl\nBZfffQfPPWdLOEOHwuGH21HR6dPh55/jHtr48bB6ta0+qGV1clECISJJrX//f2ZlSDbkywctW1pv\niXXrrHIxXz4b7FW2rJ3mePRR2Lw55qGsW2erR926QZUqMX85iTIlECKS1AoVsiFbzz9v31hLBA44\nwOZwLFz4T/vsH3+EDh2s5Wf79vD009bZKQYGDbLcRU2jkpMSCBFJem3aQNOm1v05Rp91qa9sWejV\nC5YutWEjN95o/STatbNkomNHy9C2bYvKy61YYWM/hg61AyOSfJRAiEjSc85OMH7zjc2cklw6+mgY\nONBOa3z8MfTrZy20zzkHypSBSy/NVTKxY4fNNalc2XZOJDkpgRCRlFCpkvUSGD7cEgmJkkqVbI/h\no4/sNEefPvD225ZMHHywDfd6+mnYsiXbT3nffbBkiU3dzJs3dqFLbCmBEJGUMXiwNWe85prQkaSo\nqlV3Tyb697ceE+3aQenScN55MHPmPrtfrl9vJ2cuuQROPz1+oUv0KYEQkZRxwAEwciQ8/jgsWhQ6\nmhRXtaoNr3jvPfj0U6uZ+OYbuPhiW5k480yYOhW+/363y66/3rYwRo4MFLdEjRIIEUkpF15o39l2\n7Qp//BE6mjRx7LFWM/HWW/8Uovz5pxU6HHoonHQS3H47Kx76kHvv9dx2m5VSSHJTAiEiKcU5myH1\nzTfqUBlE+fLQsye89JIdCZ0xAw47DD98OLU6VmNNgQpc9Ukf+/pff4WOVnJBCYSIpJzjjrMeAyNG\n2Ba9BHLQQVbs8OSTTBi8nrPdPAqc3Yw8Tz4BjRtDqVLwn/9YkvHTT6GjlQgpgRCRlHTddTbd8cor\nbc9dwvnqK7hhWEGO6t6C0k9Msfkby5dbEeZXX9mx0DJlbAT5LbfY1/SHlvCUQIhISipQAO6+2/oi\nTZ0aOpr05b2N6C5VCm67LfPOnbM5hgyxuom1a+Hee60n+ciRUKeO/fyyy6yt9oYNIX8LkgUlECKS\nsk491VYgrr/eOjVL/N17r43qvvtuOyWzV2XLQqdO8OSTliy88oqtSixfbm21S5eGk0+Gm26CN96A\n7dvj+VuQLCiBEJGUduedUKSIncrwPnQ06eW772yXolMnG9edLfny2TGaO++0ApZvv4V77oEjjrCx\nq/Xr23JGu3a2tPTFF/qDDUQJhIiktOLF7VTG3LlWqyfx4b21qS5cOJftxQ89FDp3hsces0LLN9+0\nTmHr19tpjwoVrPX2FVfYY9avj9rvQfZNCYSIpLxWrWxFvFcvq9+T2Js1y8ZlTJ0KJUpE6UkzMuDE\nE61p1auv2nbHs8/aH/Abb9j00NKloUYNm9/x3HPw669RenHZk/NJvvTjnKsFLF++fDm1atUKHY6I\nJKiNG+H44220wwsvWB2fxMbatfZeN21qiUTcfP+99Zd46SUrvPjmG0s6ateGhg3tduqp+yjGSD8r\nVqygdu3aALW99ysiuVYrECKSFooXt4K+RYt0KiOWduywmod8+axkIa7KlbNW2vfdB6tXw+efw5Qp\nts3x0EPQooUth5x4Ilx7ra1Q7GNuh+yb5qCJSNpo1syKKa+5xn5+zDGhI0o9kybBggXw3/9arWMw\nztkf8DHHWH2E9/DZZ/Dyy/C//8Ejj8CoUfa4E06A007751a2bMDAk4e2MEQkrfz+u31elC1rnyMa\nJx09778PdevCVVfB+PGho9kP7+HLL+0vwWuv2e2LL+xrFSrYaY9TT7UfK1VK2T2v3Gxh6J+OiKSV\nokVtNbtBA2srMHx46IhSw9atcNFF9tl7xx2ho8mGXVcoOne2+77/3hKJxYvh9dftL8qOHdaSu149\nOOUUu9Wta2eD05wSCBFJO6ecYh2TBw2CRo2gSZPQESW/G26wqd5vvw2FCoWOJofKlbPZHP/5j/36\nt9/s2Ojrr8OSJdab4tdfrTCzenVrbrXzVrFiyq5SZEUJhIikpeuus+3wiy+Gd9/VeOncmD8fxo61\nfg/Vq4eOJooOOMCyy50Z5t9/w8cf25HRpUutY+aUKfa1yZOtwCaNKIEQkbSUJ4+tUNeoYUnEggV2\nn0Rm9Wp7/1q2hD59QkcTYxkZUK2a3a680u7buNHmeVSuHDa2APTPRUTSVpky8PDD1jIgKfbtE8zW\nrXDeeXDggZaMpWUCVry4HekpXz50JHGXjn/cIiL/r3Fj278fPNh6REj29ekDH3xgM7Ci1m1SkoYS\nCBFJe0OH2jb3BRf8c5JP9u3BB23GyF132WRuST9KIEQk7WVkwKOPQsmS0Lq1Fd9L1t591wZlde4M\nl18eOhoJRQmEiAi2BP/MMzY+4dJL7fi//Nv338PZZ0OVKtZ1UtKXEggRkUxVqsDMmTBnDtx6a+ho\nEs/mzXDOOfbzuXOTuN+DRIUSCBGRXZxzjjWZuukm29YQ8/ff1mly1SqbQVWuXOiIJDT1gRAR2cMN\nN9gHZceOVhfRtGnoiMK79lpbdZg712aJiGgFQkRkD87BPffYyYy2bWHZstARhTVpknWaHD/eJmKL\ngBIIEZG9ypcPHn/cmg6edZatSKSjBx6Anj2hb1/o0SN0NJJIlECIiGShSBGYNw9KlYLmze0EQjp5\n9FE7pnnllTbnQmRXSiBERPahZEmbk7F9OzRsCN9+Gzqi+Hj6aZtxcfHFNi8qzQZNSjYogRAR2Y/D\nD7fBi9u2QYMGNkAqlT3/vE20PvdcuPfeNJ1xIfulvxYiItlwzDHw6qv2YdqgAXz+eeiIYuOxx6BN\nGyuWfPhhyKuzepIFJRAiItl0xBHwv/9B4cKWRHz8ceiIomvCBOjQwWaCzJ5thaQiWVECISISgUMP\ntSSiZEmoXx9eeil0RLnnPQwcCL17Q//+MGMG5M8fOipJdEogREQiVKYMvPYa1K0LzZpZkWGy2rbN\nhmLdcYedtBg5UjUPkj36ayIikgPFi9sRz+7doVs365GwfXvoqCLz9ddw2mnwyCNW79CvX+iIJJko\ngRARyaG8ea0749SpMG1acvWKmDcPataEn36CxYttzoVIJJRAiIjk0lVXwQsvWFFltWp2kiFRbd9u\nsz7OPttqOFasgDp1QkclyUgJhIhIFDRqBO+/b4O32re37+h/+SV0VLt7801LFkaMsJqHZ56BEiVC\nRyXJSglEipk1a1boEBKC3od/6L0w8XgfSpa09s8zZ1ozpuOPh4cegh07Yv7S+7Rpk9Vq1KsHmzbN\nYulSuO46FUvq30buxOyvj3PuBufcYufcZufczxFcN8w5971zbotzbqFzrkKsYkxF+gdh9D78Q++F\nidf74BxceKGtRpx0ko0Er1MHXnwxLi+/mz//tE6SlSrZ0cyxY6FatVnassikfxu5E8v8Mx8wG8j2\nASfn3HVAD+BK4ERgM7DAOacTySKSVA47DJ58El5/HQoWtNHgZ51l3Sy9j+1rb9liTaGOOQa6dPmn\n6VXv3pppIdETswTCez/Uez8eeD+Cy3oDt3jvn/PefwB0BMoBbWIRo4hIrNWvb6ccnnjCZmicfjpU\nrTIFiP0AAAa8SURBVGqnN6JdI/HhhzBkCBx5pB3JbNTI7nvsMUtoRKIpYXbAnHNHAWWB/1/o897/\nCrwJ1AsVl4hIbjlng6k++sg6Vx5/PFxzDZQrZ3MnJk60D/pIVyb++gtWroSbboIqVewEyPjxcN55\n8Nlntm1RpUpsfk8iiTQmpSzggXV73L8u82tZKQjwcao1pc+hTZs2sWLFitBhBKf34R96L0yivA/F\nilkB4+WXw3PP2epEv352vPKgg2x1okyZf27Fi1u3yG3bYOtW+O03+OILWLUKvvzSrita1FY2rrwS\nTj7Z2lD/8sveVzgS5X1IBHovdvvsLBjptc5HkPI6524HrtvHQzxQ2Xu/apdrLgXGeu8P2s9z1wNe\nB8p579ftcv9jwA7vfYcsrrsQmJnt34SIiIjs6SLv/SORXBDpCsQo4P79PObLCJ9zpx8AB5Rh91WI\nMsDKfVy3ALgIWA1szeFri4iIpKOCwJHYZ2lEIkogvPcbgA2Rvkg2n/sr59wPQGPgPQDn3IHAScBd\n+4kpoqxJRERE/t8bObkoln0gyjvnTgCOADKccydk3ors8phPnHOtd7lsHHCjc+4c59zxwAzgW+CZ\nWMUpIiIikYtlEeUw7BjmTjsrVRoBr2b+vCJQbOcDvPcjnHOFgWlAceA14Czv/Z8xjFNEREQiFFER\npYiIiAgkUB8IERERSR5KIERERCRiSZ9AOOe6O+e+cs794Zxb6pyrGzqmeHPOneace9Y5951zbodz\nrlXomEJwzg10zr3lnPvVObfOOfe0c+7Y0HHFm3Puaufcu865TZm3N5xzZ4aOKzTn3PWZ/z7GhI4l\n3pxzN2X+3ne9fRQ6rhCcc+Wccw8559ZnDm181zlXK3Rc8Zb5ubnn34kdzrmJ2X2OpE4gnHP/AUYD\nNwE1gXex4VulggYWf0WAd4BuWDOvdHUaMBE7+tsEG+j2gnOuUNCo4m8N1vCtFlAbeAl4xjlXOWhU\nAWV+Y3El9n9EuvoA66tTNvN2athw4s85VxxYDGwDmgOVgf5AlKeSJIU6/PN3oSzQFPv8mJ3dJ0jq\nIkrn3FLgTe9978xfO+w/zwne+xFBgwvEObcDaOO9fzZ0LKFlJpI/Ag2896+Hjick59wG4Brv/f4a\nwaUc51xRYDnQFRgMrPTe9wsbVXw5524CWnvv0+477V055+4A6nnvTw8dS6Jxzo0DWnjvs71qm7Qr\nEM65fNh3V7sO3/LAIjR8S0xxLKP+OXQgoTjn8jjn2gOFgSWh4wnkLmCu9/6l0IEEVjFzm/ML59zD\nzrnyoQMK4BxgmXNuduY25wrnXJfQQYWW+Xl6EXBvJNclbQIBlAIyiHz4lqSBzNWoccDr3vu02+t1\nzlVzzv2GLdVOBtp67z8JHFbcZSZPNYCBoWMJbClwGbZsfzVwFPDqro390sTR2ErUp0AzYAowwTl3\nSdCowmuL9WR6MJKLEmkap0g0TQaqAPVDBxLIJ8AJ2H8K5wEznHMN0imJcM4dhiWRTbz3f4WOJyTv\n/a5zDj5wzr0FfA1cwP7nG6WSPMBb3vvBmb9+1zlXDUuqHgoXVnCdgfne+x8iuSiZVyDWA39jRUG7\nKoMN5pI05ZybBLQAGnrv14aOJwTv/Xbv/Zfe+5Xe+0FY8WDv0HHFWW2gNLDCOfeXc+4v4HSgt3Pu\nz8xVqrTkvd8ErAIqhI4lztYCH+9x38fA4QFiSQjOucOxovPpkV6btAlE5ncUy7HhW8D/L1s3JoeD\nQST5ZSYPrYFG3vtvQseTQPIABUIHEWeLgOOxLYwTMm/LgIeBE3wyV5DnUmZhaQXsAzWdLAaO2+O+\n47DVmHTVGdv6fz7SC5N9C2MM8IBzbjnwFtAXKxZ7IGRQ8Za5j1kBG4cOcHTmILOfvfdrwkUWX865\nyUAHoBWw2Tm3c3Vqk/c+bUa9O+duA+YD3wAHYMVRp2N7vmnDe78Z2K3+xTm3Gdjgvd/zu9CU5pwb\nCczFPigPBYYCfwGzQsYVwFhgsXNuIHZc8SSgC3BF0KgCyfym+zLgAe/9jkivT+oEwns/O/Oo3jBs\n6+IdoLn3/qewkcVdHeBl7MSBx3pjgBXEdA4VVABXY7//V/a4vxM22TVdHIz92R8CbALeA5rpFAKQ\nvn1SDgMeAUoCPwGvAyd77zcEjSrOvPfLnHNtgTuwI71fAb2994+GjSyYJkB5clgHk9R9IERERCSM\npK2BEBERkXCUQIiIiEjElECIiIhIxJRAiIiISMSUQIiIiEjElECIiIhIxJRAiIiISMSUQIiIiEjE\nlECIiIhIxJRAiIiISMSUQIiIiEjE/g/AcdoagB7MAAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f73e6816d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y_true, c='b')\n",
    "plt.plot(x, y_pred, c='r')\n",
    "plt.show()"
=======
      "text/plain": [
       "[[784, 100], [100], [100], [100], [100, 10], [10]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.get_shape().as_list() for w in model2.trainable_weights]"
>>>>>>> 3b1bb8aecb2bc0b2db49e2affb5fd8b17f253612
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
