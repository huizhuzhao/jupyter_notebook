{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download(url, filename):\n",
    "    \"\"\" utility to download necessary data \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(filename, \"wb\") as handle:\n",
    "        for data in response.iter_content():\n",
    "            handle.write(data)\n",
    "            \n",
    "url1 = \"https://github.com/Microsoft/CNTK/blob/master/Examples/Tutorials/SLUHandsOn/atis.%s.ctf?raw=true\"\n",
    "url2 = \"https://github.com/Microsoft/CNTK/blob/master/Examples/Text/ATIS/%s.wl?raw=true\"\n",
    "urls = [url1%\"train\", url1%\"test\", url2%\"query\", url2%\"slots\"]\n",
    "\n",
    "def data():\n",
    "    for t in urls:\n",
    "        filename = t.split('/')[-1].split('?')[0]\n",
    "        try:\n",
    "            f = open(filename)\n",
    "            f.close()\n",
    "        except IOError:\n",
    "            download(t, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from cntk.blocks import default_options, LSTM, Placeholder, Input        # building blocks\n",
    "from cntk.layers import Embedding, Recurrence, Dense, BatchNormalization # layers\n",
    "from cntk.models import Sequential                                       # higher level things\n",
    "from cntk.utils import ProgressPrinter, log_number_of_parameters\n",
    "from cntk.io import MinibatchSource, CTFDeserializer\n",
    "from cntk.io import StreamDef, StreamDefs, INFINITELY_REPEAT, FULL_DATA_SWEEP\n",
    "from cntk import future_value, combine, Trainer, cross_entropy_with_softmax, classification_error, splice\n",
    "from cntk.learner import sgd, momentum_sgd, adagrad, adam_sgd, nesterov\n",
    "from cntk.learner import learning_rate_schedule, momentum_schedule, momentum_schedule_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of words in vocab, slot labels, and intent labels\n",
    "vocab_size = 943 ; num_labels = 129 ; num_intents = 26    \n",
    "\n",
    "# model dimensions\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 150\n",
    "hidden_dim = 300\n",
    "\n",
    "def create_model():\n",
    "    with default_options(initial_state=0.1):\n",
    "        return Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            Recurrence(LSTM(hidden_dim), go_backwards=False),\n",
    "            Dense(num_labels)\n",
    "        ])\n",
    "\n",
    "def create_reader(path, is_training):\n",
    "    return MinibatchSource(CTFDeserializer(path, StreamDefs(\n",
    "         query         = StreamDef(field='S0', shape=vocab_size,  is_sparse=True),\n",
    "         intent_unused = StreamDef(field='S1', shape=num_intents, is_sparse=True),  \n",
    "         slot_labels   = StreamDef(field='S2', shape=num_labels,  is_sparse=True)\n",
    "     )), randomize=is_training, epoch_size = INFINITELY_REPEAT if is_training else FULL_DATA_SWEEP)\n",
    "\n",
    "def create_learner(parameters, minibatch_size, epoch_size):\n",
    "    lr_schedule = [0.003]*4+[0.0015]*24+[0.0003]\n",
    "    m_schedule_const = [700]\n",
    "    m_schedule_float = [np.exp(-1.*minibatch_size/x) for x in m_schedule_const]\n",
    "\n",
    "    lr_1 = learning_rate_schedule(lr_schedule, units=epoch_size)\n",
    "    m_1 = momentum_schedule(           m_schedule_const, units=1)\n",
    "    m_2 = momentum_schedule_per_sample(m_schedule_float, units=1)\n",
    "\n",
    "    learner_1 = sgd(parameters, lr=lr_1)\n",
    "    learner_2 = adagrad(parameters, lr=lr_1)\n",
    "    learner_3 = momentum_sgd(parameters, lr=lr_1, momentum=m_1)\n",
    "    learner_4 = adam_sgd(parameters, lr_per_sample=lr_1, momentum_time_constant=m_1)\n",
    "    learner_5 = nesterov(parameters, lr=lr_1, momentum=m_1)\n",
    "    \n",
    "    return learner_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 721479 parameters in 6 parameter tensors.\n",
      "Finished Epoch [1]: [Training] loss = 4.839873 * 140, metric = 72.9% * 140\n",
      "[64, 64]\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "reader = create_reader(\"/home/xtalpi/git_test/test_data/examples/atis/atis.train.ctf\", is_training=True)\n",
    "\n",
    "def create_criterion_function(model):\n",
    "    labels = Placeholder()\n",
    "    ce   = cross_entropy_with_softmax(model, labels)\n",
    "    errs = classification_error      (model, labels)\n",
    "    return combine ([ce, errs]) # (features, labels) -> (loss, metric)\n",
    "\n",
    "# Importantly, for sequential data, a sample is an individual item of a sequence. \n",
    "# Hence, CNTK's minibatchSize does not refer to the number of sequences in the minibatch, \n",
    "# but the aggregate number of sequence items/tokens across the sequences that constitute the minibatch\n",
    "# details refer to https://github.com/Microsoft/CNTK/wiki/SGD-Block#what-is-the-minibatch-size-in-cntk\n",
    "def train(reader, model, max_epochs=16):\n",
    "    criterion = create_criterion_function(model)\n",
    "    criterion.replace_placeholders({criterion.placeholders[0]: Input(vocab_size),\n",
    "                                    criterion.placeholders[1]: Input(num_labels)})\n",
    "    epoch_size = 18000        # 18000 samples is half the dataset size \n",
    "    minibatch_size = 80\n",
    "    \n",
    "    learner = create_learner(criterion.parameters, minibatch_size, epoch_size)\n",
    "\n",
    "    trainer = Trainer(model=model, \n",
    "                      loss_function=criterion.outputs[0], \n",
    "                      eval_function=criterion.outputs[1], \n",
    "                      parameter_learners=learner)\n",
    "    \n",
    "    log_number_of_parameters(model)\n",
    "    progress_printer = ProgressPrinter(tag='Training')\n",
    "    \n",
    "\n",
    "   \n",
    "    t = 0\n",
    "    for ii in range(1):\n",
    "        epoch_end = (ii+1)*100\n",
    "        while t < epoch_end:\n",
    "            data = reader.next_minibatch(minibatch_size, input_map={\n",
    "                    criterion.arguments[0]: reader.streams.query,\n",
    "                    criterion.arguments[1]: reader.streams.slot_labels\n",
    "                })\n",
    "           \n",
    "            t += data[criterion.arguments[0]].num_samples\n",
    "            trainer.train_minibatch(data)\n",
    "            progress_printer.update_with_trainer(trainer, with_metric=True)\n",
    "            \n",
    "            \n",
    "        loss, metric, samples = progress_printer.epoch_summary(with_metric=True)\n",
    "        print ([data[x].num_samples for x in criterion.arguments])\n",
    "        #print (help(criterion))\n",
    "        \n",
    "\n",
    "train(reader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-fed5c6ff40a2>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-fed5c6ff40a2>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Training 721479 parameters in 6 parameter tensors.\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Training 721479 parameters in 6 parameter tensors.\n",
    "Finished Epoch [1]: [Training] loss = 1.109854 * 18029, metric = 21.0% * 18029\n",
    "[70, 70]\n",
    "Finished Epoch [2]: [Training] loss = 0.461001 * 17992, metric = 10.3% * 17992\n",
    "[73, 73]\n",
    "Finished Epoch [3]: [Training] loss = 0.305768 * 18030, metric = 6.5% * 18030\n",
    "[67, 67]\n",
    "Finished Epoch [4]: [Training] loss = 0.222351 * 18021, metric = 5.0% * 18021\n",
    "[80, 80]\n",
    "Finished Epoch [5]: [Training] loss = 0.163784 * 17946, metric = 3.8% * 17946\n",
    "[74, 74]\n",
    "Finished Epoch [6]: [Training] loss = 0.149376 * 18035, metric = 3.3% * 18035\n",
    "[70, 70]\n",
    "Finished Epoch [7]: [Training] loss = 0.123693 * 17961, metric = 2.7% * 17961\n",
    "[78, 78]\n",
    "Finished Epoch [8]: [Training] loss = 0.125279 * 17996, metric = 2.7% * 17996\n",
    "[73, 73]\n",
    "Finished Epoch [9]: [Training] loss = 0.085364 * 18038, metric = 1.8% * 18038\n",
    "[80, 80]\n",
    "Finished Epoch [10]: [Training] loss = 0.087199 * 18011, metric = 2.0% * 18011\n",
    "[77, 77]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'UnitType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0a0647381577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcntk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnitType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'UnitType'"
     ]
    }
   ],
   "source": [
    "def evaluate(reader, model):\n",
    "    criterion = create_criterion_function(model)\n",
    "    criterion.replace_placeholders({criterion.placeholders[0]: Input(num_labels)})\n",
    "\n",
    "    # process minibatches and perform evaluation\n",
    "    lr_schedule = learning_rate_schedule(1)\n",
    "    momentum_as_time_constant = momentum_as_time_constant_schedule(0)\n",
    "    dummy_learner = adam_sgd(criterion.parameters, \n",
    "                             lr=lr_schedule, momentum=momentum_as_time_constant, low_memory=True)\n",
    "    evaluator = Trainer(model, criterion.outputs[0], criterion.outputs[1], dummy_learner)\n",
    "    progress_printer = ProgressPrinter(tag='Evaluation')\n",
    "\n",
    "    while True:\n",
    "        minibatch_size = 1000\n",
    "        data = reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "            criterion.arguments[0]: reader.streams.query,\n",
    "            criterion.arguments[1]: reader.streams.slot_labels\n",
    "        })\n",
    "        if not data:                                 # until we hit the end\n",
    "            break\n",
    "        metric = evaluator.test_minibatch(data)\n",
    "        progress_printer.update(0, data[criterion.arguments[1]].num_samples, metric) # log progress\n",
    "    loss, metric, actual_samples = progress_printer.epoch_summary(with_metric=True)\n",
    "\n",
    "    return loss, metric\n",
    "\n",
    "evaluate(reader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cntk-py34]",
   "language": "python",
   "name": "conda-env-cntk-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
